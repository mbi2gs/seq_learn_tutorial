{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "level-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "severe-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['H', 'L']\n",
    "nucleotides = ['A', 'C', 'G', 'T']\n",
    "\n",
    "def encode_seq(symbols, seqtype='dna'):\n",
    "    encdr = nucleotides\n",
    "    if seqtype != 'dna':\n",
    "        encdr = states\n",
    "    outseq = np.array([encdr.index(s) for s in symbols])\n",
    "    return outseq\n",
    "\n",
    "test_hl = 'HHHLLL'\n",
    "test_nuc = 'GGGAAA'\n",
    "assert encode_seq(test_hl, seqtype='states')[0] == states.index(test_hl[0]) and \\\n",
    "       encode_seq(test_hl, seqtype='states')[-1] == states.index(test_hl[-1])\n",
    "assert encode_seq(test_nuc, seqtype='dna')[0] == nucleotides.index(test_nuc[0]) and \\\n",
    "       encode_seq(test_nuc, seqtype='dna')[-1] == nucleotides.index(test_nuc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "artificial-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(num_array, seqtype='dna'):\n",
    "    encdr = nucleotides\n",
    "    if seqtype != 'dna':\n",
    "        encdr = states\n",
    "    outseq = [encdr[s] for s in num_array]\n",
    "    return ''.join(outseq)\n",
    "\n",
    "assert decode_seq(encode_seq(test_nuc)) == test_nuc\n",
    "assert decode_seq(encode_seq(test_hl, seqtype='hid'), seqtype='hid') == test_hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pleasant-jonathan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dna</th>\n",
       "      <th>hidden_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TGGTCGTATTTTGTCGGGGGCAGACCAAAAAACAACGAAACGAATG...</td>\n",
       "      <td>LLLLLLLHHLLLHHLHHHLLHLLHHLLHLLHLHLLLLLHHHHLLLH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCACGGTGGATGTATCGCTGTGCAAGCAAGCCGGGATACTGCTTGT...</td>\n",
       "      <td>HHHHHHHHHHLHLHHHLLLHLHHLHHHHLLHLHLLLHHLLLLHHLH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 dna  \\\n",
       "0  TGGTCGTATTTTGTCGGGGGCAGACCAAAAAACAACGAAACGAATG...   \n",
       "1  GCACGGTGGATGTATCGCTGTGCAAGCAAGCCGGGATACTGCTTGT...   \n",
       "\n",
       "                                        hidden_state  \n",
       "0  LLLLLLLHHLLLHHLHHHLLHLLHHLLHLLHLHLLLLLHHHHLLLH...  \n",
       "1  HHHHHHHHHHLHLHHHLLLHLHHLHHHHLLHLHLLLHHLLLLHHLH...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_file = 'rnn_toy_training.tsv'\n",
    "training_df = pd.read_csv(training_data_file, sep='\\t')\n",
    "training_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legal-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = training_df.dna.str.len().max()\n",
    "NUM_SEQS = training_df.shape[0]\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "X_train = torch.zeros(NUM_SEQS, SEQ_LEN, dtype=torch.long)\n",
    "Y_train = torch.zeros(NUM_SEQS, SEQ_LEN, dtype=torch.long)\n",
    "for i, row in training_df.iterrows():\n",
    "    dna = row['dna']\n",
    "    hid = row['hidden_state']\n",
    "    dna_encode = torch.LongTensor(encode_seq(dna, seqtype='dna'))\n",
    "    hid_encode = torch.LongTensor(encode_seq(hid, seqtype='hid'))\n",
    "\n",
    "    X_train[i, :] = dna_encode\n",
    "    Y_train[i, :] = hid_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hungry-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 500]), torch.Size([1000, 500]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "canadian-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train[:50,:], Y_train[:50,:])\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "difficult-miami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 500]), torch.Size([1, 500]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polish-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, predict_size, n_layers=1, bdir=False):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.predict_size = predict_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bdir else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, self.embed_size)\n",
    "        self.gru = nn.GRU(self.embed_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=n_layers, \n",
    "                          bidirectional=bdir)\n",
    "\n",
    "        self.lin_out = nn.Linear(hidden_size*self.n_directions, predict_size)\n",
    "        self.sigmoid = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        # embedding shape: (batch_size, seq_len, hidden_size)\n",
    "        # transpose so that batch dim is in the 2nd index position\n",
    "        output = torch.transpose(embedded, 0, 1)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output shape: (seq_len, batch_size, n_directions*hidden_size)\n",
    "        # hidden shape: (n_directions*n_layers, batch_size, hidden_size)\n",
    "        \n",
    "        output = self.sigmoid(self.lin_out(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(self.n_layers*self.n_directions, \n",
    "                           batch_size, \n",
    "                           self.hidden_size)\n",
    "    \n",
    "    def input_dims(self):\n",
    "        print(f'Input dimensions are: (batch_size, seq_len, {self.input_size})')\n",
    "    \n",
    "    def output_dims(self):\n",
    "        print(f'Output dimensions are: (seq_len, batch_size, {self.predict_size})')\n",
    "    \n",
    "    def hidden_dims(self):\n",
    "        dnl = self.n_layers*self.n_directions\n",
    "        print(f'Hidden dimensions are: ({dnl}, batch_size, {self.hidden_size})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "falling-protection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions are: (batch_size, seq_len, 4)\n",
      "Hidden dimensions are: (1, batch_size, 10)\n",
      "Output dimensions are: (seq_len, batch_size, 2)\n"
     ]
    }
   ],
   "source": [
    "test_model = GRU(len(nucleotides), 10, len(states))\n",
    "test_model.input_dims(), test_model.hidden_dims(), test_model.output_dims();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "refined-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, \n",
    "          learn_rate=0.02, \n",
    "          input_dim=len(nucleotides), \n",
    "          hidden_dim=10,\n",
    "          output_dim=len(states),\n",
    "          batch_size=1,\n",
    "          EPOCHS=5):\n",
    "    \n",
    "    # Instantiating the model\n",
    "    model = GRU(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting training\")\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        start_time = time.time()\n",
    "        avg_loss = 0\n",
    "        for sample_x, sample_y in train_loader:\n",
    "            h = model.init_hidden(batch_size)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # the heart of the training!\n",
    "            out, h = model(sample_x, h)\n",
    "            \n",
    "            # NLLLoss expects batch first, then class probabilities, then seq_len\n",
    "            out_T = torch.transpose(out, 0, 1)\n",
    "            out_T = out_T.transpose(1, 2)\n",
    "\n",
    "            loss = criterion(out_T, sample_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "        current_time = time.time()\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} Done, Total Loss: {avg_loss/len(train_loader):.3f}\")\n",
    "        print(f\"Total Time Elapsed: {current_time-start_time:.1f} seconds\")\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(f\"Total Training Time: {str(sum(epoch_times))} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/5 Done, Total Loss: 0.672\n",
      "Total Time Elapsed: 5.1 seconds\n",
      "Epoch 2/5 Done, Total Loss: 0.668\n",
      "Total Time Elapsed: 5.1 seconds\n"
     ]
    }
   ],
   "source": [
    "gru_model = train(train_loader, learn_rate = 0.02, EPOCHS=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dna):\n",
    "    assert all([x in nucleotides for x in dna])\n",
    "    assert isinstance(model, GRU)\n",
    "    dna_encode = torch.LongTensor(encode_seq(dna, seqtype='dna'))\n",
    "    dna_encode = dna_encode[None, :]\n",
    "    h = model.init_hidden(1)\n",
    "    model.zero_grad()\n",
    "    out, _ = model(dna_encode, h)\n",
    "    out_state_indices = [int(torch.argmax(x)) for x in out[:,0]]\n",
    "    out_probs = np.array([torch.exp(x).detach().numpy() for x in out[:,0]])\n",
    "    state = decode_seq(out_state_indices, 'hid')\n",
    "    return state, out_probs\n",
    "\n",
    "test_seq = 'GGGTTT'\n",
    "test_state, test_probs = predict(gru_model, test_seq)\n",
    "assert len(test_state) == len(test_seq)\n",
    "assert all([x in states for x in test_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hl, pred_prob_hl = predict(gru_model, training_df.iloc[-1, 0])\n",
    "\n",
    "def align(seq1, seq2):\n",
    "    WIDTH = 60\n",
    "    lines = int(np.ceil(len(seq1) / WIDTH))\n",
    "    match = ''\n",
    "    for i, c1 in enumerate(seq1):\n",
    "        indicator = ' '\n",
    "        if c1 != seq2[i]:\n",
    "            indicator = '*'\n",
    "        match += indicator\n",
    "    \n",
    "    for i in range(lines):\n",
    "        print('Seq1', seq1[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('    ', match[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('Seq2', seq2[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print()\n",
    "align(training_df.iloc[-1, 1], pred_hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_hl[:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-happiness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
