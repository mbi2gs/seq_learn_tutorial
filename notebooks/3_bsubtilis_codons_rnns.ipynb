{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "serial-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-toronto",
   "metadata": {},
   "source": [
    "#### Codon prediction using RNNs\n",
    "\n",
    "In this notebook we go through the process of training a recurrent neural network (RNN) to learn to approximate the codon use distribution of *Bacillus subtilis*.\n",
    "\n",
    "Instead of downloading and reading in the *B. subtilis* genome, we will use the same training and test sequences selected in notebook 1. Recall that a random 20% of the genes from the genome were set aside for the purpose of testing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data from notebook 1\n",
    "with open('test_translations.pickle', 'rb') as handle:\n",
    "    test_translations = pickle.load(handle)\n",
    "    \n",
    "with open('train_translations.pickle', 'rb') as handle:\n",
    "    train_translations = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-transaction",
   "metadata": {},
   "source": [
    "Some helper functions to split a nucleotide sequence into codons and check for ambiguous codons or sequences with lengths that aren't a multiple of 3.\n",
    "\n",
    "As always, `assert` statements are little tests to make sure that things are working the way we expect. They are *very* helpful for catching silly bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpine-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_codons(dna_seq):\n",
    "    codons = []\n",
    "    for i in range(0, len(dna_seq), 3):\n",
    "        codons.append(dna_seq[i:i+3])\n",
    "    return codons\n",
    "assert get_list_of_codons('ATGCCCGGGAAATTTTAG') == ['ATG', 'CCC', 'GGG', 'AAA', 'TTT', 'TAG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-emission",
   "metadata": {},
   "source": [
    "Notice that we include a new symbol for padding sequences of different lengths (\"&\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "settled-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_SYMB = '&'\n",
    "u_aas = set([STOP_SYMB])\n",
    "u_codons = set([STOP_SYMB])\n",
    "translations = {}\n",
    "for protein in train_translations.keys():\n",
    "    nucleotide_seq = train_translations[protein]\n",
    "    aas = {aa for aa in protein}\n",
    "    codon = get_list_of_codons(nucleotide_seq)\n",
    "    if len(protein) == len(codon):\n",
    "        u_aas = u_aas.union(aas)\n",
    "        u_codons = u_codons.union(set(codon))\n",
    "lu_aas = list(u_aas)\n",
    "lu_codons = list(u_codons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-european",
   "metadata": {},
   "source": [
    "Familiar helper functions to encode and decode sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "offshore-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(seq_obj, seqtype='dna'):\n",
    "    encdr = lu_codons\n",
    "    symbols = get_list_of_codons(seq_obj)\n",
    "    if seqtype != 'dna':\n",
    "        encdr = lu_aas\n",
    "        symbols = [c for c in seq_obj]\n",
    "    outseq = np.array([encdr.index(s) for s in symbols])\n",
    "    return outseq\n",
    "\n",
    "test_aa = 'MENILD'\n",
    "test_nuc = 'AAAAAAATAAGATAG'\n",
    "assert encode_seq(test_aa, seqtype='prot')[0] == lu_aas.index(test_aa[0]) and \\\n",
    "       encode_seq(test_aa, seqtype='prot')[-1] == lu_aas.index(test_aa[-1])\n",
    "assert encode_seq(test_nuc, seqtype='dna')[0] == lu_codons.index(test_nuc[0:3]) and \\\n",
    "       encode_seq(test_nuc, seqtype='dna')[-1] == lu_codons.index(test_nuc[-3:])\n",
    "\n",
    "def decode_seq(num_array, seqtype='dna'):\n",
    "    encdr = lu_codons\n",
    "    if seqtype != 'dna':\n",
    "        encdr = lu_aas\n",
    "    outseq = [encdr[s] for s in num_array]\n",
    "    return ''.join(outseq)\n",
    "\n",
    "assert decode_seq(encode_seq(test_nuc)) == test_nuc\n",
    "assert decode_seq(encode_seq(test_aa, seqtype='prot'), seqtype='prot') == test_aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-preservation",
   "metadata": {},
   "source": [
    "Our data set will consist of PyTorch tensors. These are simply numerical matrices (like you would find in Numpy or MATLAB or R), but they include the ability to track gradients.\n",
    "\n",
    "This case is more complex than the toy example because all the sequences of different lengths. For now we will deal with this by padding the empty space at the end of a sequence with a stop symbol \"&\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecological-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = max([len(x) for x in test_translations] \\\n",
    "            + [len(x) for x in train_translations])\n",
    "BATCH_SIZE = 1\n",
    "ENC_STOP_DNA = encode_seq(STOP_SYMB, seqtype='dna')[0]\n",
    "ENC_STOP_PROT = encode_seq(STOP_SYMB, seqtype='prot')[0]\n",
    "np.random.seed(2021)\n",
    "\n",
    "# Convert dictionary of protein/DNA pairs into tensors\n",
    "def seq_dict_to_tensors(seq_dict,\n",
    "                        seq_len,\n",
    "                        enc_stop_dna,\n",
    "                        enc_stop_prot):\n",
    "    assert len(seq_dict) > 0\n",
    "    \n",
    "    X = torch.ones(len(seq_dict),\n",
    "                   seq_len,\n",
    "                   dtype=torch.long)*enc_stop_prot\n",
    "    Y = torch.ones(len(seq_dict),\n",
    "                   seq_len,\n",
    "                   dtype=torch.long)*enc_stop_dna\n",
    "    for i, prot in enumerate(seq_dict):\n",
    "        codons = seq_dict[prot]\n",
    "        prot_encode = torch.LongTensor(encode_seq(prot, seqtype='prot'))\n",
    "        codn_encode = torch.LongTensor(encode_seq(codons, seqtype='dna'))\n",
    "\n",
    "        X[i, :len(prot_encode)] = prot_encode\n",
    "        Y[i, :len(prot_encode)] = codn_encode\n",
    "    return X, Y\n",
    "\n",
    "X_test, Y_test = seq_dict_to_tensors(test_translations,\n",
    "                                     SEQ_LEN,\n",
    "                                     ENC_STOP_DNA,\n",
    "                                     ENC_STOP_PROT)\n",
    "X_train, Y_train = seq_dict_to_tensors(train_translations,\n",
    "                                     SEQ_LEN,\n",
    "                                     ENC_STOP_DNA,\n",
    "                                     ENC_STOP_PROT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disciplinary-dividend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3389, 5489]), torch.Size([3389, 5489]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "entitled-survey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([847, 5489]), torch.Size([847, 5489]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-steam",
   "metadata": {},
   "source": [
    "Next, we load the data into a `Dataset` and `Dataloader` module. This is not strictly necessary for training, but it makes it easier to shuffle, sample from and batch the data. When projects get more complicated, these modules are very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "classical-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_data = TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-director",
   "metadata": {},
   "source": [
    "Here we make sure the data comes back out of the dataloader in the way we expect. The `batch_size` refers to the number of examples sampled simultaneously. In this case, we only retrieve one example sequence at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sweet-opinion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5489]), torch.Size([1, 5489]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the data comes back out in the way we expect\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-headset",
   "metadata": {},
   "source": [
    "Here we set up a [python class](https://www.geeksforgeeks.org/python-classes-and-objects/) to manage the various pieces of our model. Again, this is not strictly necessary, but it makes life simpler. For example, the class will track useful information internally (e.g. `input_size`, `n_layers`). We can also create convenience functions such as `init_hidden()` to create a fresh hidden layer without having to remember what the precise dimensions ought to be. \n",
    "\n",
    "There are two pieces to our class. The `__init__()` function that is run when we first instantiate an instance of `MyGruClass`. This is where we initialize variables with the correct values, and instantiate the machine learning layers. A Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN). Notice the Tensorflow GRU module is called here (which is the heart of our model). We also include an [\"embedding\" layer](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526), which learns to represent our integer inputs (remember we converted our sequence of characters into a sequence of integers using the `encode_seq()` function) as a vector of foating point numbers. That vector of floating point numbers serves as the input to our GRU. Finally, we instantiate a `Linear` and `LogSoftMax` layer, both of which convert the GRU output into a sequence of log probabilities. \n",
    "\n",
    "The `forward()` function is where we actually *use* the layers we created with `__init()`. In `forward()` we take an input and cascade it through the layers to produce an output. The key to any kind of deep learning project is to carefully track the input and output dimensions of your layers. Notice my comments to help myself mentally track what each layer is spitting out, and what the next layer expects. It helps to be aware of the [`transpose()` function](https://pytorch.org/docs/stable/generated/torch.transpose.html), which allows you to swap (i.e. \"rotate\") two dimensions. This helps to match a tensor with the input expections of a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "substantial-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGruClass(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, predict_size, n_layers=1, bdir=False):\n",
    "        super(MyGruClass, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.predict_size = predict_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bdir else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, self.embed_size)\n",
    "        self.gru = nn.GRU(self.embed_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=n_layers, \n",
    "                          bidirectional=bdir)\n",
    "\n",
    "        self.lin_out = nn.Linear(hidden_size*self.n_directions, predict_size)\n",
    "        self.sigmoid = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        # embedding shape: (batch_size, seq_len, hidden_size)\n",
    "        # transpose so that batch dim is in the 2nd index position\n",
    "        output = torch.transpose(embedded, 0, 1)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output shape: (seq_len, batch_size, n_directions*hidden_size)\n",
    "        # hidden shape: (n_directions*n_layers, batch_size, hidden_size)\n",
    "        \n",
    "        output = self.sigmoid(self.lin_out(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(self.n_layers*self.n_directions, \n",
    "                           batch_size, \n",
    "                           self.hidden_size)\n",
    "\n",
    "    def input_dims(self):\n",
    "        print(f'Input dimensions are: (batch_size, seq_len, {self.input_size})')\n",
    "    \n",
    "    def output_dims(self):\n",
    "        print(f'Output dimensions are: (seq_len, batch_size, {self.predict_size})')\n",
    "    \n",
    "    def hidden_dims(self):\n",
    "        dnl = self.n_layers*self.n_directions\n",
    "        print(f'Hidden dimensions are: ({dnl}, batch_size, {self.hidden_size})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stable-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions are: (batch_size, seq_len, 22)\n",
      "Hidden dimensions are: (1, batch_size, 10)\n",
      "Output dimensions are: (seq_len, batch_size, 65)\n"
     ]
    }
   ],
   "source": [
    "# Using convenience functions in MyGruClass to print expected dimensions\n",
    "test_model = MyGruClass(len(lu_aas), 10, len(lu_codons))\n",
    "test_model.input_dims(), test_model.hidden_dims(), test_model.output_dims();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-nylon",
   "metadata": {},
   "source": [
    "The `train()` function is where the action happens. Here we instantiate our model, and start feeding data to it. We select the Negative Log-Likelihood Loss `NLLLoss()` [function](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) as our optimization function because it determines whether a log probality (the output of our GRU) correctly classifies the input. In other words, the more probability the model assigns to the correct category, the more the the model is \"rewarded\". \n",
    "\n",
    "In our case, the model will output a vector of 65 probabilities (log transformed), one each possible codon (plus the buffer symbol \"&\"). For every input amino acid, the model will assign the probability that it corresponds to each codon. The training data also has the true codons from the genom to which the model can compare its prediction. The more probability assigned to the correct codon, the more the current model weights are reinforced.\n",
    "\n",
    "There are many loss functions, all intended for different scenarios or that have different emphases. When starting a new project, it's worth reviewing the [available loss functions](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7) to pick the one or two that seem most appropriate.\n",
    "\n",
    "PyTorch also offers [multiple optimization algorithms](https://pytorch.org/docs/stable/optim.html). For this project we went with a common default: Adam. \n",
    "\n",
    "Having initialized training data, a model, a loss function, and an optimizer, we are ready to learn. The training process proceeds to loop through the data set in a random order (because we set the `shuffle` parameter on our Dataloader to `True`). A full loop through the data is called an \"epoch\". Within each epoch, we iterate through every training batch (in this case, batches are just one sequence long). Before feeding a sequence to our RNN, we reset the hidden state and the gradient. We then feed the training sequence to our model, and collect the prediction output. The output is then reorganized (using `transpose()`) to match the input expections of our NLLLoss function. Once we get a loss value, we call `backward()` to calculate derivatives, and which are fed to the optimization function, which updates the model weights. It's remarkable how much PyTorch keeps track of for us.\n",
    "\n",
    "Finally, there are some print statements to keep track of where we are in the loop, and whether the model is continuing to improve or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "royal-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_buffer(x, y, stop_symbol):\n",
    "    buf_seq_locs = np.where(x.numpy() == ENC_STOP_PROT)\n",
    "    if buf_seq_locs[0].shape[0] > 0:\n",
    "        end_of_seq = buf_seq_locs[1][0]\n",
    "        x = x[:, :end_of_seq]\n",
    "        y = y[:, :end_of_seq]\n",
    "    return x, y\n",
    "\n",
    "def get_test_loss(model, loader, batch_size):\n",
    "    '''Calculate cumulative loss over test data set.'''\n",
    "    \n",
    "    print(\"   Testing...\")\n",
    "    criterion = nn.NLLLoss()\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    for sample_x, sample_y in loader:\n",
    "        h = model.init_hidden(batch_size)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Remove buffer symbols\n",
    "        sample_x, sample_y = rm_buffer(sample_x, sample_y, ENC_STOP_PROT)\n",
    "\n",
    "        # the heart of the training!\n",
    "        out, h = model(sample_x, h)\n",
    "\n",
    "        # NLLLoss expects batch first, then class probabilities, then seq_len\n",
    "        out_T = torch.transpose(out, 0, 1)\n",
    "        out_T = out_T.transpose(1, 2)\n",
    "\n",
    "        loss = criterion(out_T, sample_y)\n",
    "        avg_loss += loss.item()\n",
    "    return avg_loss\n",
    "\n",
    "def train(loader_training,\n",
    "          loader_testing,\n",
    "          learn_rate=0.02, \n",
    "          input_dim=len(lu_aas), \n",
    "          hidden_dim=10,\n",
    "          output_dim=len(lu_codons),\n",
    "          batch_size=1,\n",
    "          EPOCHS=5):\n",
    "    \n",
    "    # Instantiating the model\n",
    "    model = MyGruClass(input_dim, hidden_dim, output_dim, n_layers=1)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    print(\"Starting training\")\n",
    "    epoch_losses = {'epoch':[], 'train_loss':[], 'test_loss':[]}\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        test_loss = get_test_loss(model, loader_testing, batch_size)\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for sample_x, sample_y in loader_training:\n",
    "            h = model.init_hidden(batch_size)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Remove buffer symbols\n",
    "            sample_x, sample_y = rm_buffer(sample_x, \n",
    "                                           sample_y, \n",
    "                                           ENC_STOP_PROT)\n",
    "            # the heart of the training!\n",
    "            out, h = model(sample_x, h)\n",
    "            \n",
    "            # NLLLoss expects batch first, then class probabilities,\n",
    "            # then seq_len\n",
    "            out_T = torch.transpose(out, 0, 1)\n",
    "            out_T = out_T.transpose(1, 2)\n",
    "            \n",
    "            # Calculate loss function, back propagate\n",
    "            loss = criterion(out_T, sample_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss / sample and store results\n",
    "        ave_train_loss = train_loss/len(loader_training)\n",
    "        ave_test_loss = test_loss/len(loader_testing)\n",
    "        current_time = time.time()\n",
    "        epoch_losses['epoch'].append(epoch)\n",
    "        epoch_losses['train_loss'].append(ave_train_loss)\n",
    "        epoch_losses['test_loss'].append(ave_test_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}/{EPOCHS} Done')\n",
    "        print(f'  Training Loss: {ave_train_loss:.3f}')\n",
    "        print(f'  Testing Loss: {ave_test_loss:.3f}')\n",
    "        print(f\"  Total Time Elapsed: {current_time-start_time:.1f} seconds\")\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(f\"Total Training Time: {sum(epoch_times):.1f} seconds\")\n",
    "    \n",
    "    return model, pd.DataFrame(epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-noise",
   "metadata": {},
   "source": [
    "And now, for the big moment. We train the model! \n",
    "\n",
    "This will take a about 20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "attached-television",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "   Testing...\n",
      "Epoch 1/3 Done\n",
      "  Training Loss: 1.118\n",
      "  Testing Loss: 4.165\n",
      "  Total Time Elapsed: 210.9 seconds\n",
      "   Testing...\n",
      "Epoch 2/3 Done\n",
      "  Training Loss: 1.092\n",
      "  Testing Loss: 1.110\n",
      "  Total Time Elapsed: 186.6 seconds\n",
      "   Testing...\n",
      "Epoch 3/3 Done\n",
      "  Training Loss: 1.090\n",
      "  Testing Loss: 1.093\n",
      "  Total Time Elapsed: 186.8 seconds\n",
      "Total Training Time: 584.3 seconds\n"
     ]
    }
   ],
   "source": [
    "gru_model, loss_history = train(train_loader, \n",
    "                                test_loader, \n",
    "                                learn_rate = 0.02, \n",
    "                                EPOCHS=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-sequence",
   "metadata": {},
   "source": [
    "Training proceeded nicely. We can visualize the loss function value for the training and test sets over time to get a sense of when additional training is no longer useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "overall-announcement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history['epoch'], \n",
    "         loss_history['train_loss'], 'r', \n",
    "         loss_history['test_loss'], 'b--')\n",
    "plt.legend(['Training loss', 'Testing loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.suptitle('Loss over epochs', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-oasis",
   "metadata": {},
   "source": [
    "It looks like very little progress is made after 3 epochs.\n",
    "\n",
    "Let's see how the GRU's predictions compare to a real example!\n",
    "\n",
    "First, we need a function to handle all the steps of encoding and decoding the output and resetting the model. This function outputs both the predicted state (a string of H's and L's), and the associated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "occupied-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, protein):\n",
    "    assert all([x in u_aas for x in protein])\n",
    "    assert isinstance(model, MyGruClass)\n",
    "    prot_encode = torch.LongTensor(encode_seq(protein, seqtype='prot'))\n",
    "    prot_encode = prot_encode[None, :]\n",
    "    h = model.init_hidden(1)\n",
    "    model.zero_grad()\n",
    "    out, _ = model(prot_encode, h)\n",
    "    out_state_indices = [int(torch.argmax(x)) for x in out[:,0]]\n",
    "    out_probs = np.array([torch.exp(x).detach().numpy() for x in out[:,0]])\n",
    "    codons = decode_seq(out_state_indices, 'dna')\n",
    "    return codons, out_probs\n",
    "\n",
    "test_aa = 'MENILD'\n",
    "test_nucleotides, test_probs = predict(gru_model, test_aa)\n",
    "assert len(test_nucleotides) == len(test_aa)*3\n",
    "test_codons = get_list_of_codons(test_nucleotides)\n",
    "assert all([x in u_codons for x in test_codons])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-regard",
   "metadata": {},
   "source": [
    "Here we pick a sequence from our testing data that the model has never seen. We can align the prediction and the HMM generated sequence to see how closely they agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "essential-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq1 ATGCACGTGTCAACATTTCAAGCATTAATGCTTATGCTTGCTTTCGGGTCATTTATAATT\n",
      "          *  ****        *  ** *              *  *  ****     *   \n",
      "Seq2 ATGCATGTAAGCACATTTCAGGCGCTTATGCTTATGCTTGCGTTTGGAAGCTTTATTATT\n",
      "\n",
      "Seq1 GCCCTGTTGACTTATATAAAGAAGAAATAG\n",
      "       *  ** *  *     *  *  *     *\n",
      "Seq2 GCGCTTCTTACATATATTAAAAAAAAATAA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_protein = list(test_translations.keys())[0]\n",
    "test_dna = test_translations[test_protein]\n",
    "pred_dna, pred_prob_dna = predict(gru_model, test_protein)\n",
    "\n",
    "def align(seq1, seq2, WIDTH=60):\n",
    "    '''Align two input sequences of equal length,\n",
    "    with *  between indicating mismatches.'''\n",
    "    lines = int(np.ceil(len(seq1) / WIDTH))\n",
    "    match = ''\n",
    "    for i, c1 in enumerate(seq1):\n",
    "        indicator = ' '\n",
    "        if c1 != seq2[i]:\n",
    "            indicator = '*'\n",
    "        match += indicator\n",
    "    \n",
    "    for i in range(lines):\n",
    "        print('Seq1', seq1[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('    ', match[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('Seq2', seq2[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print()\n",
    "\n",
    "align(test_dna, pred_dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-perception",
   "metadata": {},
   "source": [
    "That's a nice visual, to see so many nucleotides matching! But how does it look quantitatively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afraid-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN fraction bps matching: 0.722\n"
     ]
    }
   ],
   "source": [
    "def fraction_matches(seq1, seq2):\n",
    "    '''Compare sequences in terms of matching positions.'''\n",
    "    matches = 0\n",
    "    for i, c1 in enumerate(seq1):\n",
    "        if c1 == seq2[i]:\n",
    "            matches += 1\n",
    "    return {'length':len(seq1), \n",
    "            'n_matches':matches, \n",
    "            'n_mismatches':len(seq1) - matches, \n",
    "            'fraction_matches':matches/len(seq1)}\n",
    "\n",
    "fm_rnn = fraction_matches(test_dna, pred_dna)\n",
    "\n",
    "print(f'RNN fraction bps matching: {fm_rnn[\"fraction_matches\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-limit",
   "metadata": {},
   "source": [
    "So 80% of predicted nucleotides match those actually found to code for this protein. Is that good model performance?\n",
    "\n",
    "One comparison would be to compare our predictions with random DNA sequences, but that's not very interesting. A better comparison is to benchmark with a standard approach to codon optimization: which is, to always predict the most common codon for any given amino acid.\n",
    "\n",
    "For that, we'll return to our codon usage table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equal-wages",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amino_acid</th>\n",
       "      <th>codon</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>V</td>\n",
       "      <td>GTG</td>\n",
       "      <td>0.257938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>V</td>\n",
       "      <td>GTT</td>\n",
       "      <td>0.287079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>W</td>\n",
       "      <td>TGG</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Y</td>\n",
       "      <td>TAC</td>\n",
       "      <td>0.347468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Y</td>\n",
       "      <td>TAT</td>\n",
       "      <td>0.652532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amino_acid codon  frequency\n",
       "64          V   GTG   0.257938\n",
       "65          V   GTT   0.287079\n",
       "66          W   TGG   1.000000\n",
       "67          Y   TAC   0.347468\n",
       "68          Y   TAT   0.652532"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_table = pd.read_csv('train_codon_usage.tsv', sep='\\t')\n",
    "cu_table.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "employed-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_codons(protein, codon_usage_table):\n",
    "    '''Return a DNA sequence using the most frequent codons.'''\n",
    "    mlaa = (codon_usage_table\n",
    "            .sort_values(by='frequency', ascending=False)\n",
    "            .drop_duplicates(subset=['amino_acid'])\n",
    "           )\n",
    "    dna = ''\n",
    "    for aa in protein:\n",
    "        dna += mlaa.query(f'amino_acid == \"{aa}\"').codon.values[0]\n",
    "    return dna\n",
    "\n",
    "assert get_most_common_codons('YWY', cu_table) == 'TATTGGTAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thick-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CU fraction bps matching: 0.811\n"
     ]
    }
   ],
   "source": [
    "mle_dna = get_most_common_codons(test_protein, cu_table)\n",
    "fm_cu = fraction_matches(test_dna, mle_dna)\n",
    "print(f'CU fraction bps matching: {fm_cu[\"fraction_matches\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-external",
   "metadata": {},
   "source": [
    "For this example, the codon usage outperformed our model! \n",
    "\n",
    "What about for a larger test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unlike-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_N = min(100, len(test_translations))\n",
    "rnn_fms = np.zeros(TEST_N)\n",
    "cu_fms = np.zeros(TEST_N)\n",
    "for i, test_protein in enumerate(test_translations.keys()):\n",
    "    if i >= TEST_N:\n",
    "        break\n",
    "        \n",
    "    test_dna = test_translations[test_protein]\n",
    "    \n",
    "    # HMM\n",
    "    pred_dna, pred_prob_dna = predict(gru_model, test_protein)\n",
    "    fm_rnn = fraction_matches(test_dna, pred_dna)\n",
    "    rnn_fms[i] = fm_rnn[\"fraction_matches\"]\n",
    "    \n",
    "    # Codon usage\n",
    "    cu_dna = get_most_common_codons(test_protein, cu_table)\n",
    "    fm_cu = fraction_matches(test_dna, cu_dna)\n",
    "    cu_fms[i] = fm_cu[\"fraction_matches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "indirect-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN: 0.780, CU: 0.794\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN: {np.mean(rnn_fms):.3f}, CU: {np.mean(cu_fms):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adaptive-courtesy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rnn_fms, 10, facecolor='blue', alpha=0.5, label='RNN')\n",
    "plt.hist(cu_fms, 10, facecolor='orange', alpha=0.5, label='CU')\n",
    "plt.legend()\n",
    "plt.xlabel('Fraction nucleotide matches')\n",
    "plt.ylabel('Counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "biological-flexibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999913198584"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, pval = ttest_rel(rnn_fms, cu_fms, alternative = \"greater\")\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-conversation",
   "metadata": {},
   "source": [
    "The maximum codon usage approach is outperforming our RNN by a statistically significant margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-screen",
   "metadata": {},
   "source": [
    "I guess we need to search for better hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-conversion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
