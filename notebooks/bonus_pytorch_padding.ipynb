{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "associate-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-radius",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate how to group proteins into batches. Training models on batches of data tends to make model learning \"smoother\". That is, the model updates its weights to maximize performance on the entire batch, rather than a single example, thus better approximating the learning trend we are seeking on the whole data set. \n",
    "\n",
    "The operations of \"padding\" and \"packing\" are not well documented by PyTorch, so I created the following example based on this very helpful blog post by [Sia Xin Yun Suzanna](https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html).\n",
    "\n",
    "All of this beginning stuff looks familiar. We read in a bunch of proteins as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/045/GCF_000009045.1_ASM904v1/GCF_000009045.1_ASM904v1_genomic.gbff.gz'\n",
    "gz_fn = 'GCF_000009045.1_ASM904v1_genomic.gbff.gz'\n",
    "gbk_fn = gz_fn.replace('.gbff.gz', '.gbk')\n",
    "if not os.path.isfile(gz_fn):\n",
    "    print('Beginning file download with urllib2...')\n",
    "    urllib.request.urlretrieve(url, gz_fn)\n",
    "\n",
    "if not os.path.isfile(gbk_fn):\n",
    "    with gzip.open(gz_fn, 'rb') as f_in, open(gbk_fn, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "def get_list_of_codons(dna_seq):\n",
    "    codons = []\n",
    "    for i in range(0, len(dna_seq), 3):\n",
    "        codons.append(dna_seq[i:i+3])\n",
    "    return codons\n",
    "assert get_list_of_codons('ATGCCCGGGAAATTTTAG') == ['ATG', 'CCC', 'GGG', 'AAA', 'TTT', 'TAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diagnostic-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_proteins = []\n",
    "host_codons = []\n",
    "\n",
    "u_aas = set()\n",
    "u_codons = set()\n",
    "all_codons = []\n",
    "initial_states = []\n",
    "emissions = {}\n",
    "for record in SeqIO.parse(gbk_fn, \"genbank\"):\n",
    "    for feature in record.features:\n",
    "        if feature.type == 'CDS' and 'translation' in feature.qualifiers:\n",
    "            protein = feature.qualifiers['translation'][0] + '*'\n",
    "            host_proteins.append(protein)\n",
    "            \n",
    "            aas = set([aa for aa in protein])\n",
    "            codon = get_list_of_codons(str(feature.extract(record.seq)))\n",
    "            \n",
    "            host_codons.append(codon)\n",
    "            all_codons.append(codon)\n",
    "            initial_states.append(codon[0])\n",
    "            u_aas = u_aas.union(aas)\n",
    "            u_codons = u_codons.union(set(codon))\n",
    "            for i, cdn in enumerate(codon):\n",
    "                emissions[cdn] = protein[i]\n",
    "lu_aas = ['0'] + list(u_aas)\n",
    "lu_codons = ['PAD'] + list(u_codons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organizational-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(seq_obj, seqtype='dna'):\n",
    "    encdr = lu_codons\n",
    "    symbols = get_list_of_codons(seq_obj)\n",
    "    if seqtype != 'dna':\n",
    "        encdr = lu_aas\n",
    "        symbols = [c for c in seq_obj]\n",
    "    outseq = np.array([encdr.index(s) for s in symbols])\n",
    "    return outseq\n",
    "\n",
    "test_aa = 'MENILD0'\n",
    "test_nuc = 'AAAAAAATAAGATAGPAD'\n",
    "assert encode_seq(test_aa, seqtype='prot')[0] == lu_aas.index(test_aa[0]) and \\\n",
    "       encode_seq(test_aa, seqtype='prot')[-1] == lu_aas.index(test_aa[-1])\n",
    "assert encode_seq(test_nuc, seqtype='dna')[0] == lu_codons.index(test_nuc[0:3]) and \\\n",
    "       encode_seq(test_nuc, seqtype='dna')[-1] == lu_codons.index(test_nuc[-3:])\n",
    "\n",
    "def decode_seq(num_array, seqtype='dna'):\n",
    "    encdr = lu_codons\n",
    "    if seqtype != 'dna':\n",
    "        encdr = lu_aas\n",
    "    outseq = [encdr[s] for s in num_array]\n",
    "    return ''.join(outseq)\n",
    "\n",
    "assert decode_seq(encode_seq(test_nuc)) == test_nuc\n",
    "assert decode_seq(encode_seq(test_aa, seqtype='prot'), seqtype='prot') == test_aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-alaska",
   "metadata": {},
   "source": [
    "#### Pytorch padding\n",
    "\n",
    "Think of the sequences as one hot vectors stacked up, \n",
    "so that the dimensions are (seq_len, num_characters)\n",
    "\n",
    "Given a dictionary of characters of length 300, we'll \n",
    "pad and pack 3 sequences together of lengths 25, 22, and 15\n",
    "The final object will have the length of the longest sequence (25),\n",
    "a batch size of 3 (the number of sequences) and a dictionary \n",
    "size of 300 (the number of possible characters in our sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developmental-installation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 300])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(25, 300)\n",
    "b = torch.ones(22, 300)\n",
    "c = torch.ones(15, 300)\n",
    "d = pad_sequence([a, b, c])\n",
    "d.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-porcelain",
   "metadata": {},
   "source": [
    "Now that you understand the gist of padding, here we create a custom `Dataset` object that pads a set of proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tight-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "class ProteinsSet(Dataset):\n",
    "    \"\"\"Protein data set\"\"\"\n",
    "    \n",
    "    def __init__(self, list_of_proteins, list_of_codons, codon_list, aa_list):\n",
    "        self.prot_collection = list_of_proteins\n",
    "        self.codon_collection = list_of_codons\n",
    "        self.lu_codons = codon_list\n",
    "        self.lu_aas = aa_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prot_collection)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        encd_prot = self.__encode__(self.prot_collection[idx], 'prot')\n",
    "        encd_codn = self.__encode__(self.codon_collection[idx], 'dna')\n",
    "        return (encd_prot, encd_codn)\n",
    "    \n",
    "    def __encode__(self, seq_obj, seqtype):\n",
    "        encdr = self.lu_codons\n",
    "        symbols = seq_obj\n",
    "        if seqtype != 'dna':\n",
    "            encdr = self.lu_aas\n",
    "            symbols = [c for c in seq_obj]\n",
    "        encoded_seq = []\n",
    "        t = torch.zeros(len(symbols), len(encdr))\n",
    "        for i, s in enumerate(symbols):\n",
    "            t[i, encdr.index(s)] = 1\n",
    "        return t\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # turn list of tuples into two lists: \n",
    "    # https://stackoverflow.com/a/8081590 \n",
    "    [xx, yy] = map(list, zip(*batch))\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "    xx_pad = pad_sequence(xx, batch_first=False, padding_value=0)\n",
    "    yy_pad = pad_sequence(yy, batch_first=False, padding_value=0)\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "\n",
    "ps = ProteinsSet(host_proteins, host_codons, lu_codons, lu_aas)\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(dataset=ps, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=True, \n",
    "                         collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exotic-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X training data torch.Size([501, 16, 22])\n",
      "size of Y training data torch.Size([501, 16, 65])\n",
      "Max sequence lengths 501\n",
      "Max sequence lengths 501\n"
     ]
    }
   ],
   "source": [
    "for (x_padded, y_padded, x_lengths, y_lengths) in data_loader:\n",
    "    print('size of X training data', x_padded.shape)\n",
    "    print('size of Y training data', y_padded.shape)\n",
    "    print('Max sequence lengths', max(x_lengths))\n",
    "    print('Max sequence lengths', max(y_lengths))\n",
    "    break\n",
    "assert x_padded.shape[0] == max(x_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-quantum",
   "metadata": {},
   "source": [
    "Here we load a single batch. \n",
    "\n",
    "The first dimension is the length of the longest sequence.\n",
    "The batch size (second dimension) is 16 sequences. \n",
    "The third dimension is the dictionary length (22 amino acids or 65 codons).\n",
    "\n",
    "The x and y lengths objects are lists of the sequence lengths so that the model knows where the padding begins for each training example. The maximum sequence lengths match the first dimension of the training data, which is what we expect.\n",
    "\n",
    "#### Packing your padded sequences\n",
    "\n",
    "Packing is done for computational efficiency. By packing the batch of sequences, the RNN doesn't have to operate on so many uninformative padd characters. There is a very nice visualization and motivation at this [Stack Overflow answer](https://stackoverflow.com/a/56211056). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "timely-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(lu_aas)\n",
    "h_dim = len(lu_codons)\n",
    "n_layers = 2\n",
    "rnn = nn.GRU(embedding_dim, h_dim, n_layers, batch_first=False)\n",
    "\n",
    "# Pack for efficiency\n",
    "x_packed = pack_padded_sequence(x_padded, \n",
    "                                x_lengths, \n",
    "                                batch_first=False, \n",
    "                                enforce_sorted=False)\n",
    "# Push through RNN\n",
    "h0 = torch.zeros(n_layers, batch_size, h_dim)\n",
    "output_packed, h1 = rnn(x_packed, h0)\n",
    "\n",
    "# Then unpack\n",
    "output_padded, output_lengths = pad_packed_sequence(output_packed, \n",
    "                                                    batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thick-trace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([501, 16, 65])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_padded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "verbal-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_padded.shape[0] == output_padded.shape[0]\n",
    "assert x_padded.shape[1] == output_padded.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-business",
   "metadata": {},
   "source": [
    "Notice that the output object has the same sequence length and batch size (16), and the only difference is the output dimension, which matches the number of codons (65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-lightning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
