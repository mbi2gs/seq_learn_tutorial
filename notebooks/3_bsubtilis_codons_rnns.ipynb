{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-generator",
   "metadata": {},
   "source": [
    "In this notebook we go through the process of training a recurrent neural network (RNN) to learn to approximate the codon use distribution of *Bacillus subtilis*.\n",
    "\n",
    "First, we download the *B. subtilis genome*. Much of this first bit is repeated from the HMM example in `1_bsubtilis_codons_hmms.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "human-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/045/GCF_000009045.1_ASM904v1/GCF_000009045.1_ASM904v1_genomic.gbff.gz'\n",
    "gz_fn = 'GCF_000009045.1_ASM904v1_genomic.gbff.gz'\n",
    "gbk_fn = gz_fn.replace('.gbff.gz', '.gbk')\n",
    "if not os.path.isfile(gz_fn):\n",
    "    print('Beginning file download with urllib2...')\n",
    "    urllib.request.urlretrieve(url, gz_fn)\n",
    "\n",
    "if not os.path.isfile(gbk_fn):\n",
    "    with gzip.open(gz_fn, 'rb') as f_in, open(gbk_fn, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-experiment",
   "metadata": {},
   "source": [
    "Some helper functions to split a nucleotide sequence into codons and check for ambiguous codons or sequences with lengths that aren't a multiple of 3.\n",
    "\n",
    "As always, `assert` statements are little tests to make sure that things are working the way we expect. They are *very* helpful for catching silly bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handed-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_codons(dna_seq):\n",
    "    codons = []\n",
    "    for i in range(0, len(dna_seq), 3):\n",
    "        codons.append(dna_seq[i:i+3])\n",
    "    return codons\n",
    "assert get_list_of_codons('ATGCCCGGGAAATTTTAG') == ['ATG', 'CCC', 'GGG', 'AAA', 'TTT', 'TAG']\n",
    "\n",
    "def check_len_and_ambiguity(seq):\n",
    "    assert isinstance(seq, str)\n",
    "    ambig_nucs = ['R', 'Y', 'S', 'W', 'K', 'M', 'B', 'D', 'H', 'V', 'N']\n",
    "    unambiguous = not any([anuc in seq for anuc in ambig_nucs])\n",
    "    multiple_3 = len(seq) % 3 == 0\n",
    "    return unambiguous and multiple_3\n",
    "assert check_len_and_ambiguity('ATGACCTAG')\n",
    "assert not check_len_and_ambiguity('ATGACCTA')\n",
    "assert not check_len_and_ambiguity('ATGACCTAY')\n",
    "assert not check_len_and_ambiguity('ATGACCAY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-treasury",
   "metadata": {},
   "source": [
    "And now we read in all the protein coding sequences from the genome. During this process, we keep track of the set of unique amino acids and nucleotides, the initial states, and---of great importance---the codons that correspond to each amino acid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suited-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_SYMB = '&'\n",
    "u_aas = set([STOP_SYMB])\n",
    "u_codons = set([STOP_SYMB])\n",
    "translations = {}\n",
    "for record in SeqIO.parse(gbk_fn, \"genbank\"):\n",
    "    for feature in record.features:\n",
    "        if feature.type == 'CDS' and \\\n",
    "           'translation' in feature.qualifiers:\n",
    "            if check_len_and_ambiguity(str(record.seq)):\n",
    "                protein = feature.qualifiers['translation'][0] + '*'\n",
    "                aas = {aa for aa in protein}\n",
    "                nucleotide_seq = str(feature.extract(record.seq))\n",
    "                codon = get_list_of_codons(nucleotide_seq)\n",
    "                if len(protein) == len(codon):\n",
    "                    u_aas = u_aas.union(aas)\n",
    "                    u_codons = u_codons.union(set(codon))\n",
    "                    translations[protein] = nucleotide_seq\n",
    "#         if len(translations) >= 300:\n",
    "#             break\n",
    "lu_aas = list(u_aas)\n",
    "lu_codons = list(u_codons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-singapore",
   "metadata": {},
   "source": [
    "Familiar helper functions to encode and decode sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mysterious-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(seq_obj, seqtype='dna'):\n",
    "    encdr = lu_codons\n",
    "    symbols = get_list_of_codons(seq_obj)\n",
    "    if seqtype != 'dna':\n",
    "        encdr = lu_aas\n",
    "        symbols = [c for c in seq_obj]\n",
    "    outseq = np.array([encdr.index(s) for s in symbols])\n",
    "    return outseq\n",
    "\n",
    "test_aa = 'MENILD'\n",
    "test_nuc = 'AAAAAAATAAGATAG'\n",
    "assert encode_seq(test_aa, seqtype='prot')[0] == lu_aas.index(test_aa[0]) and \\\n",
    "       encode_seq(test_aa, seqtype='prot')[-1] == lu_aas.index(test_aa[-1])\n",
    "assert encode_seq(test_nuc, seqtype='dna')[0] == lu_codons.index(test_nuc[0:3]) and \\\n",
    "       encode_seq(test_nuc, seqtype='dna')[-1] == lu_codons.index(test_nuc[-3:])\n",
    "\n",
    "def decode_seq(num_array, seqtype='dna'):\n",
    "    encdr = lu_codons\n",
    "    if seqtype != 'dna':\n",
    "        encdr = lu_aas\n",
    "    outseq = [encdr[s] for s in num_array]\n",
    "    return ''.join(outseq)\n",
    "\n",
    "assert decode_seq(encode_seq(test_nuc)) == test_nuc\n",
    "assert decode_seq(encode_seq(test_aa, seqtype='prot'), seqtype='prot') == test_aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-snake",
   "metadata": {},
   "source": [
    "Our data set will consist of PyTorch tensors. These are simply numerical matrices (like you would find in Numpy or MATLAB or R), but they include the ability to track gradients.\n",
    "\n",
    "This case is more complex than the toy example because all the sequences of different lengths. For now we will deal with this by padding the empty space at the end of a sequence with a stop symbol \"&\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quality-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = np.max([len(key) for key in translations.keys()])\n",
    "NUM_SEQS = len(translations)\n",
    "BATCH_SIZE = 1\n",
    "ENC_STOP_DNA = encode_seq(STOP_SYMB, seqtype='dna')[0]\n",
    "ENC_STOP_PROT = encode_seq(STOP_SYMB, seqtype='prot')[0]\n",
    "TEST_FRACTION = 0.2\n",
    "np.random.seed(2021)\n",
    "\n",
    "# Split protein/DNA pairs into random training and test set\n",
    "test_indices = choice(NUM_SEQS, \n",
    "                      size=int(np.floor(TEST_FRACTION*NUM_SEQS)), \n",
    "                      replace=False)\n",
    "train_indices = [x for x in range(NUM_SEQS) if x not in test_indices]\n",
    "trnslts_lst = list(translations.items())\n",
    "test_translations = dict([trnslts_lst[x] for x in test_indices])\n",
    "train_translations = dict([trnslts_lst[x] for x in train_indices])\n",
    "\n",
    "# Convert dictionary of protein/DNA pairs into tensors\n",
    "def seq_dict_to_tensors(seq_dict, \n",
    "                        seq_len, \n",
    "                        enc_stop_dna, \n",
    "                        enc_stop_prot):\n",
    "    assert len(seq_dict) > 0\n",
    "    \n",
    "    X = torch.ones(len(seq_dict), \n",
    "                   seq_len, \n",
    "                   dtype=torch.long)*enc_stop_prot\n",
    "    Y = torch.ones(len(seq_dict), \n",
    "                   seq_len, \n",
    "                   dtype=torch.long)*enc_stop_dna\n",
    "    for i, prot in enumerate(seq_dict):\n",
    "        codons = seq_dict[prot]\n",
    "        prot_encode = torch.LongTensor(encode_seq(prot, seqtype='prot'))\n",
    "        codn_encode = torch.LongTensor(encode_seq(codons, seqtype='dna'))\n",
    "\n",
    "        X[i, :len(prot_encode)] = prot_encode\n",
    "        Y[i, :len(prot_encode)] = codn_encode\n",
    "    return X, Y\n",
    "\n",
    "X_test, Y_test = seq_dict_to_tensors(test_translations,\n",
    "                                     SEQ_LEN,\n",
    "                                     ENC_STOP_DNA,\n",
    "                                     ENC_STOP_PROT)\n",
    "X_train, Y_train = seq_dict_to_tensors(train_translations,\n",
    "                                     SEQ_LEN,\n",
    "                                     ENC_STOP_DNA,\n",
    "                                     ENC_STOP_PROT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-programming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([240, 1200]), torch.Size([240, 1200]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outer-louisville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 1200]), torch.Size([60, 1200]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-madagascar",
   "metadata": {},
   "source": [
    "Next, we load the data into a `Dataset` and `Dataloader` module. This is not strictly necessary for training, but it makes it easier to shuffle, sample from and batch the data. When projects get more complicated, these modules are very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "allied-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_data = TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-import",
   "metadata": {},
   "source": [
    "Here we make sure the data comes back out of the dataloader in the way we expect. The `batch_size` refers to the number of examples sampled simultaneously. In this case, we only retrieve one example sequence at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "every-organization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1200]), torch.Size([1, 1200]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the data comes back out in the way we expect\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-model",
   "metadata": {},
   "source": [
    "Here we set up a [python class](https://www.geeksforgeeks.org/python-classes-and-objects/) to manage the various pieces of our model. Again, this is not strictly necessary, but it makes life simpler. For example, the class will track useful information internally (e.g. `input_size`, `n_layers`). We can also create convenience functions such as `init_hidden()` to create a fresh hidden layer without having to remember what the precise dimensions ought to be. \n",
    "\n",
    "There are two pieces to our class. The `__init__()` function that is run when we first instantiate an instance of `MyGruClass`. This is where we initialize variables with the correct values, and instantiate the machine learning layers. A Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN). Notice the Tensorflow GRU module is called here (which is the heart of our model). We also include an [\"embedding\" layer](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526), which learns to represent our integer inputs (remember we converted our sequence of characters into a sequence of integers using the `encode_seq()` function) as a vector of foating point numbers. That vector of floating point numbers serves as the input to our GRU. Finally, we instantiate a `Linear` and `LogSoftMax` layer, both of which convert the GRU output into a sequence of log probabilities. \n",
    "\n",
    "The `forward()` function is where we actually *use* the layers we created with `__init()`. In `forward()` we take an input and cascade it through the layers to produce an output. The key to any kind of deep learning project is to carefully track the input and output dimensions of your layers. Notice my comments to help myself mentally track what each layer is spitting out, and what the next layer expects. It helps to be aware of the [`transpose()` function](https://pytorch.org/docs/stable/generated/torch.transpose.html), which allows you to rotate two dimensions. This helps to match a tensor with the input expections of a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "isolated-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGruClass(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, predict_size, n_layers=1, bdir=False):\n",
    "        super(MyGruClass, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.predict_size = predict_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bdir else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, self.embed_size)\n",
    "        self.gru = nn.GRU(self.embed_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=n_layers, \n",
    "                          bidirectional=bdir)\n",
    "\n",
    "        self.lin_out = nn.Linear(hidden_size*self.n_directions, predict_size)\n",
    "        self.sigmoid = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        # embedding shape: (batch_size, seq_len, hidden_size)\n",
    "        # transpose so that batch dim is in the 2nd index position\n",
    "        output = torch.transpose(embedded, 0, 1)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output shape: (seq_len, batch_size, n_directions*hidden_size)\n",
    "        # hidden shape: (n_directions*n_layers, batch_size, hidden_size)\n",
    "        \n",
    "        output = self.sigmoid(self.lin_out(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(self.n_layers*self.n_directions, \n",
    "                           batch_size, \n",
    "                           self.hidden_size)\n",
    "\n",
    "    def input_dims(self):\n",
    "        print(f'Input dimensions are: (batch_size, seq_len, {self.input_size})')\n",
    "    \n",
    "    def output_dims(self):\n",
    "        print(f'Output dimensions are: (seq_len, batch_size, {self.predict_size})')\n",
    "    \n",
    "    def hidden_dims(self):\n",
    "        dnl = self.n_layers*self.n_directions\n",
    "        print(f'Hidden dimensions are: ({dnl}, batch_size, {self.hidden_size})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interior-chick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions are: (batch_size, seq_len, 22)\n",
      "Hidden dimensions are: (1, batch_size, 10)\n",
      "Output dimensions are: (seq_len, batch_size, 65)\n"
     ]
    }
   ],
   "source": [
    "# Using convenience functions in MyGruClass to print expected dimensions\n",
    "test_model = MyGruClass(len(lu_aas), 10, len(lu_codons))\n",
    "test_model.input_dims(), test_model.hidden_dims(), test_model.output_dims();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-illustration",
   "metadata": {},
   "source": [
    "The `train()` function is where the action happens. Here we instantiate our model, and start feeding data to it. We select the Negative Log-Likelihood Loss `NLLLoss()` [function](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) as our optimization function because it determines whether a log probality (the output of our GRU) correctly classifies the input. In other words, the more probability the model assigns to the correct category, the more the the model is \"rewarded\". \n",
    "\n",
    "In our case, the model will output a vector of 65 probabilities (log transformed), one each possible codon (plus the buffer symbol \"&\"). For every input amino acid, the model will assign the probability that it corresponds to each codon. The training data also has the true codons from the genom to which the model can compare its prediction. The more probability assigned to the correct codon, the more the current model weights are reinforced.\n",
    "\n",
    "There are many loss functions, all intended for different scenarios or that have different emphases. When starting a new project, it's worth reviewing the [available loss functions](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7) to pick the one or two that seem most appropriate.\n",
    "\n",
    "PyTorch also offers [multiple optimization algorithms](https://pytorch.org/docs/stable/optim.html). For this project we went with a common default: Adam. \n",
    "\n",
    "Having initialized training data, a model, a loss function, and an optimizer, we are ready to learn. The training process proceeds to loop through the data set in a random order (because we set the `shuffle` parameter on our Dataloader to `True`). A full loop through the data is called an \"epoch\". Within each epoch, we iterate through every training batch (in this case, batches are just one sequence long). Before feeding a sequence to our RNN, we reset the hidden state and the gradient. We then feed the training sequence to our model, and collect the prediction output. The output is then reorganized (using `transpose()`) to match the input expections of our NLLLoss function. Once we get a loss value, we call `backward()` to calculate derivatives, and which are fed to the optimization function, which updates the model weights. It's remarkable how much PyTorch keeps track of for us.\n",
    "\n",
    "Finally, there are some print statements to keep track of where we are in the loop, and whether the model is continuing to improve or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "major-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_buffer(x, y, stop_symbol):\n",
    "    buf_seq_locs = np.where(x.numpy() == ENC_STOP_PROT)\n",
    "    if buf_seq_locs[0].shape[0] > 0:\n",
    "        end_of_seq = buf_seq_locs[1][0]\n",
    "        x = x[:, :end_of_seq]\n",
    "        y = y[:, :end_of_seq]\n",
    "    return x, y\n",
    "\n",
    "def get_test_loss(model, loader, batch_size):\n",
    "    '''Calculate cumulative loss over test data set.'''\n",
    "    \n",
    "    print(\"   Testing...\")\n",
    "    criterion = nn.NLLLoss()\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    for sample_x, sample_y in loader:\n",
    "        h = model.init_hidden(batch_size)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Remove buffer symbols\n",
    "        sample_x, sample_y = rm_buffer(sample_x, sample_y, ENC_STOP_PROT)\n",
    "\n",
    "        # the heart of the training!\n",
    "        out, h = model(sample_x, h)\n",
    "\n",
    "        # NLLLoss expects batch first, then class probabilities, then seq_len\n",
    "        out_T = torch.transpose(out, 0, 1)\n",
    "        out_T = out_T.transpose(1, 2)\n",
    "\n",
    "        loss = criterion(out_T, sample_y)\n",
    "        avg_loss += loss.item()\n",
    "    return avg_loss\n",
    "\n",
    "def train(loader_training,\n",
    "          loader_testing,\n",
    "          learn_rate=0.02, \n",
    "          input_dim=len(lu_aas), \n",
    "          hidden_dim=10,\n",
    "          output_dim=len(lu_codons),\n",
    "          batch_size=1,\n",
    "          EPOCHS=5):\n",
    "    \n",
    "    # Instantiating the model\n",
    "    model = MyGruClass(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    print(\"Starting training\")\n",
    "    epoch_losses = {'epoch':[], 'train_loss':[], 'test_loss':[]}\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        test_loss = get_test_loss(model, loader_testing, batch_size)\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for sample_x, sample_y in loader_training:\n",
    "            h = model.init_hidden(batch_size)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Remove buffer symbols\n",
    "            sample_x, sample_y = rm_buffer(sample_x, \n",
    "                                           sample_y, \n",
    "                                           ENC_STOP_PROT)\n",
    "            # the heart of the training!\n",
    "            out, h = model(sample_x, h)\n",
    "            \n",
    "            # NLLLoss expects batch first, then class probabilities,\n",
    "            # then seq_len\n",
    "            out_T = torch.transpose(out, 0, 1)\n",
    "            out_T = out_T.transpose(1, 2)\n",
    "            \n",
    "            # Calculate loss function, back propagate\n",
    "            loss = criterion(out_T, sample_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss / sample and store results\n",
    "        ave_train_loss = train_loss/len(loader_training)\n",
    "        ave_test_loss = test_loss/len(loader_testing)\n",
    "        current_time = time.time()\n",
    "        epoch_losses['epoch'].append(epoch)\n",
    "        epoch_losses['train_loss'].append(ave_train_loss)\n",
    "        epoch_losses['test_loss'].append(ave_test_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}/{EPOCHS} Done')\n",
    "        print(f'  Training Loss: {ave_train_loss:.3f}')\n",
    "        print(f'  Testing Loss: {ave_test_loss:.3f}')\n",
    "        print(f\"  Total Time Elapsed: {current_time-start_time:.1f} seconds\")\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(f\"Total Training Time: {sum(epoch_times):.1f} seconds\")\n",
    "    \n",
    "    return model, pd.DataFrame(epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-cooling",
   "metadata": {},
   "source": [
    "And now, for the big moment. We train the model! \n",
    "\n",
    "This will take a about 30 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "progressive-amplifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "   Testing...\n",
      "Epoch 1/10 Done\n",
      "  Training Loss: 1.442\n",
      "  Testing Loss: 4.217\n",
      "  Total Time Elapsed: 13.2 seconds\n",
      "   Testing...\n",
      "Epoch 2/10 Done\n",
      "  Training Loss: 1.121\n",
      "  Testing Loss: 1.120\n",
      "  Total Time Elapsed: 13.4 seconds\n",
      "   Testing...\n",
      "Epoch 3/10 Done\n",
      "  Training Loss: 1.114\n",
      "  Testing Loss: 1.110\n",
      "  Total Time Elapsed: 13.3 seconds\n",
      "   Testing...\n",
      "Epoch 4/10 Done\n",
      "  Training Loss: 1.106\n",
      "  Testing Loss: 1.103\n",
      "  Total Time Elapsed: 13.3 seconds\n",
      "   Testing...\n",
      "Epoch 5/10 Done\n",
      "  Training Loss: 1.108\n",
      "  Testing Loss: 1.100\n",
      "  Total Time Elapsed: 13.2 seconds\n",
      "   Testing...\n",
      "Epoch 6/10 Done\n",
      "  Training Loss: 1.106\n",
      "  Testing Loss: 1.108\n",
      "  Total Time Elapsed: 13.3 seconds\n",
      "   Testing...\n",
      "Epoch 7/10 Done\n",
      "  Training Loss: 1.106\n",
      "  Testing Loss: 1.112\n",
      "  Total Time Elapsed: 13.3 seconds\n",
      "   Testing...\n",
      "Epoch 8/10 Done\n",
      "  Training Loss: 1.101\n",
      "  Testing Loss: 1.091\n",
      "  Total Time Elapsed: 13.2 seconds\n",
      "   Testing...\n",
      "Epoch 9/10 Done\n",
      "  Training Loss: 1.100\n",
      "  Testing Loss: 1.120\n",
      "  Total Time Elapsed: 13.4 seconds\n",
      "   Testing...\n",
      "Epoch 10/10 Done\n",
      "  Training Loss: 1.101\n",
      "  Testing Loss: 1.088\n",
      "  Total Time Elapsed: 13.4 seconds\n",
      "Total Training Time: 133.0 seconds\n"
     ]
    }
   ],
   "source": [
    "gru_model, loss_history = train(train_loader, \n",
    "                                test_loader, \n",
    "                                learn_rate = 0.02, \n",
    "                                EPOCHS=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-guitar",
   "metadata": {},
   "source": [
    "Training proceeded nicely. We can visualize the loss function value for the training and test sets over time to get a sense of when additional training is no longer useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exposed-uruguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history['epoch'], \n",
    "         loss_history['train_loss'], 'r', \n",
    "         loss_history['test_loss'], 'b--')\n",
    "plt.legend(['Training loss', 'Testing loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.suptitle('Loss over epochs', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-memphis",
   "metadata": {},
   "source": [
    "It looks like very little progress is made after 3 epochs.\n",
    "\n",
    "Let's see how the GRU's predictions compare to a real example!\n",
    "\n",
    "First, we need a function to handle all the steps of encoding and decoding the output and resetting the model. This function outputs both the predicted state (a string of H's and L's), and the associated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lucky-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, protein):\n",
    "    assert all([x in u_aas for x in protein])\n",
    "    assert isinstance(model, MyGruClass)\n",
    "    prot_encode = torch.LongTensor(encode_seq(protein, seqtype='prot'))\n",
    "    prot_encode = prot_encode[None, :]\n",
    "    h = model.init_hidden(1)\n",
    "    model.zero_grad()\n",
    "    out, _ = model(prot_encode, h)\n",
    "    out_state_indices = [int(torch.argmax(x)) for x in out[:,0]]\n",
    "    out_probs = np.array([torch.exp(x).detach().numpy() for x in out[:,0]])\n",
    "    codons = decode_seq(out_state_indices, 'dna')\n",
    "    return codons, out_probs\n",
    "\n",
    "test_aa = 'MENILD'\n",
    "test_nucleotides, test_probs = predict(gru_model, test_aa)\n",
    "assert len(test_nucleotides) == len(test_aa)*3\n",
    "test_codons = get_list_of_codons(test_nucleotides)\n",
    "assert all([x in u_codons for x in test_codons])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-method",
   "metadata": {},
   "source": [
    "Here we pick a sequence from our testing data that the model has never seen. We can align the prediction and the HMM generated sequence to see how closely they agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "periodic-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq1 ATGGCGCAGCACACAATAGATCAAACACAAGTAATCCACACTAAGCCCAGCGCTTTATCA\n",
      "          *  *  *     *              *     *  *  *  ****   * *  *\n",
      "Seq2 ATGGCTCAACATACAATCGATCAAACACAAGTTATCCATACAAAACCTTCTGCTCTTTCT\n",
      "\n",
      "Seq1 TATAAAGAGAAGACGCTGGTGATGGGAATTTTAAACGTAACGCCTGACTCTTTCTCGGAC\n",
      "             *  *  *  *  *     *  ** *     *  *     *     *  *  *\n",
      "Seq2 TATAAAGAAAAAACACTTGTTATGGGTATCCTTAACGTTACACCTGATTCTTTTTCTGAT\n",
      "\n",
      "Seq1 GGCGGAAAATATGACAGCTTGGACAAGGCGCTGCTGCACGCGAAAGAGATGATCGATGAT\n",
      "       *  *        ****  *  *  *  *  *  *  *  *     *            \n",
      "Seq2 GGTGGTAAATATGATTCTTTAGATAAAGCTCTTCTTCATGCTAAAGAAATGATCGATGAT\n",
      "\n",
      "Seq1 GGTGCCCATATCATTGATATTGGAGGGGAATCGACAAGGCCTGGCGCTGAGTGCGTATCT\n",
      "          *        *     *  *  *     *   * *     *     *     *   \n",
      "Seq2 GGTGCTCATATCATCGATATCGGTGGTGAATCTACACGTCCTGGTGCTGAATGCGTTTCT\n",
      "\n",
      "Seq1 GAGGATGAGGAGATGTCCAGAGTCATTCCGGTGATTGAGCGGATTACGAAAGAGCTTGGT\n",
      "       *     *  *     ** *  *  *  *  *  *  *  *  *  *     *      \n",
      "Seq2 GAAGATGAAGAAATGTCTCGTGTTATCCCTGTTATCGAACGTATCACAAAAGAACTTGGT\n",
      "\n",
      "Seq1 GTTCCTATTTCTGTAGACACGTACAAGGCTTCTGTCGCAGATGAAGCAGTGAAAGCCGGT\n",
      "             *     *  *  *  *  *        *  *        *  *     *   \n",
      "Seq2 GTTCCTATCTCTGTTGATACATATAAAGCTTCTGTTGCTGATGAAGCTGTTAAAGCTGGT\n",
      "\n",
      "Seq1 GCATCCATTATCAATGATATTTGGGGAGCCAAACATGATCCGAAGATGGCTTCCGTTGCA\n",
      "       *  *  *     *     *     *  *           *  *        *     *\n",
      "Seq2 GCTTCTATCATCAACGATATCTGGGGTGCTAAACATGATCCTAAAATGGCTTCTGTTGCT\n",
      "\n",
      "Seq1 GCTGAACATAATGTTCCAATTGTACTCATGCATAACCGCCCTGAAAGAAACTACAATGAC\n",
      "                *     *  *  *  *           *      * *     *  *  *\n",
      "Seq2 GCTGAACATAACGTTCCTATCGTTCTTATGCATAACCGTCCTGAACGTAACTATAACGAT\n",
      "\n",
      "Seq1 TTATTGCCGGATATGCTGTCGGACTTAATGGAGAGTGTAAAAATTGCTGTTGAGGCCGGA\n",
      "       ** *  *        *  *  *  *     ***   *     *        *  *  *\n",
      "Seq2 TTGCTTCCTGATATGCTTTCTGATTTGATGGAATCTGTTAAAATCGCTGTTGAAGCTGGT\n",
      "\n",
      "Seq1 GTAGACGAGAAGAACATTATTCTTGATCCTGGTATCGGTTTCGCGAAAACCTATCACGAT\n",
      "       *  *  *  *     *  *                    *  *     *     *   \n",
      "Seq2 GTTGATGAAAAAAACATCATCCTTGATCCTGGTATCGGTTTTGCTAAAACATATCATGAT\n",
      "\n",
      "Seq1 AACTTGGCAGTGATGAACAAACTAGAGATTTTCAGCGGATTGGGATATCCGGTTCTTCTG\n",
      "        * *  *  *           *  *  *  ****  ** *  *     *        *\n",
      "Seq2 AACCTTGCTGTTATGAACAAACTTGAAATCTTTTCTGGTCTTGGTTATCCTGTTCTTCTT\n",
      "\n",
      "Seq1 GCAACCTCCCGAAAAAGATTCATCGGACGTGTTCTGGATCTTCCGCCTGAGGAGCGGGCT\n",
      "       *  *  *  *   * *  *     *        *   * *  *     *  *  *   \n",
      "Seq2 GCTACATCTCGTAAACGTTTTATCGGTCGTGTTCTTGATTTGCCTCCTGAAGAACGTGCT\n",
      "\n",
      "Seq1 GAGGGCACAGGCGCGACTGTGTGTCTCGGCATTCAAAAAGGCTGTGACATTGTCAGGGTC\n",
      "       *  *     *  *  *  *  *  *  *  *        *  *  *  *  ** *  *\n",
      "Seq2 GAAGGTACAGGTGCTACAGTTTGCCTTGGTATCCAAAAAGGTTGCGATATCGTTCGTGTT\n",
      "\n",
      "Seq1 CATGATGTAAAGCAAATTGCCAGAATGGCGAAAATGATGGACGCGATGCTGAATAAGGGA\n",
      "             *  *     *  ** *     *           *  *     *  *  *  *\n",
      "Seq2 CATGATGTTAAACAAATCGCTCGTATGGCTAAAATGATGGATGCTATGCTTAACAAAGGT\n",
      "\n",
      "Seq1 GGGGTGCACCATGGATAA\n",
      "       *  *  *     *   \n",
      "Seq2 GGTGTTCATCATGGTTAA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_protein = list(test_translations.keys())[0]\n",
    "test_dna = test_translations[test_protein]\n",
    "pred_dna, pred_prob_dna = predict(gru_model, test_protein)\n",
    "\n",
    "def align(seq1, seq2, WIDTH=60):\n",
    "    '''Align two input sequences of equal length,\n",
    "    with *  between indicating mismatches.'''\n",
    "    lines = int(np.ceil(len(seq1) / WIDTH))\n",
    "    match = ''\n",
    "    for i, c1 in enumerate(seq1):\n",
    "        indicator = ' '\n",
    "        if c1 != seq2[i]:\n",
    "            indicator = '*'\n",
    "        match += indicator\n",
    "    \n",
    "    for i in range(lines):\n",
    "        print('Seq1', seq1[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('    ', match[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('Seq2', seq2[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print()\n",
    "\n",
    "align(test_dna, pred_dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "friendly-bloom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amino_acid</th>\n",
       "      <th>codon</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>V</td>\n",
       "      <td>GTG</td>\n",
       "      <td>0.233003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>V</td>\n",
       "      <td>GTT</td>\n",
       "      <td>0.315106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>W</td>\n",
       "      <td>TGG</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Y</td>\n",
       "      <td>TAC</td>\n",
       "      <td>0.370437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Y</td>\n",
       "      <td>TAT</td>\n",
       "      <td>0.629563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amino_acid codon  frequency\n",
       "61          V   GTG   0.233003\n",
       "62          V   GTT   0.315106\n",
       "63          W   TGG   1.000000\n",
       "64          Y   TAC   0.370437\n",
       "65          Y   TAT   0.629563"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_codon_usage_table(prot_dna_dict):\n",
    "    '''Returns a codon usage table.'''\n",
    "    cu_dict = {'amino_acid':[],\n",
    "               'codon':[]}\n",
    "    for prot in prot_dna_dict.keys():\n",
    "        codons = get_list_of_codons(prot_dna_dict[prot])\n",
    "        for i, aa in enumerate(prot):\n",
    "            cu_dict['amino_acid'].append(aa)\n",
    "            cu_dict['codon'].append(codons[i])\n",
    "    cu_df = pd.DataFrame(cu_dict)\n",
    "    cu_df = (cu_df\n",
    "             .groupby(['amino_acid', 'codon'])\n",
    "             .size()\n",
    "             .reset_index()\n",
    "            )\n",
    "    cu_df['sum_aa'] = (cu_df\n",
    "                       .groupby('amino_acid')[0]\n",
    "                       .transform('sum')\n",
    "                      )\n",
    "    cu_df = (cu_df\n",
    "             .assign(frequency = lambda x: x[0] / x['sum_aa'])\n",
    "            )\n",
    "    \n",
    "    return cu_df[['amino_acid', 'codon', 'frequency']]\n",
    "    \n",
    "cu_table = build_codon_usage_table(translations)\n",
    "cu_table.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "frequent-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_codons(protein, codon_usage_table):\n",
    "    '''Return a DNA sequence using the most frequent codons.'''\n",
    "    mlaa = (codon_usage_table\n",
    "            .sort_values(by='frequency', ascending=False)\n",
    "            .drop_duplicates(subset=['amino_acid'])\n",
    "           )\n",
    "    dna = ''\n",
    "    for aa in protein:\n",
    "        dna += mlaa.query(f'amino_acid == \"{aa}\"').codon.values[0]\n",
    "    return dna\n",
    "\n",
    "assert get_most_common_codons('YWY', cu_table) == 'TATTGGTAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sealed-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq1 ATGGCGCAGCACACAATAGATCAAACACAAGTAATCCACACTAAGCCCAGCGCTTTATCA\n",
      "          *  *  *     *              *  *  *  *  *  ****   * *   \n",
      "Seq2 ATGGCTCAACATACAATTGATCAAACACAAGTTATTCATACAAAACCGTCAGCTCTTTCA\n",
      "\n",
      "Seq1 TATAAAGAGAAGACGCTGGTGATGGGAATTTTAAACGTAACGCCTGACTCTTTCTCGGAC\n",
      "             *  *  *  *  *         * *  *  *  *  *  *  *  *  *  *\n",
      "Seq2 TATAAAGAAAAAACACTTGTTATGGGAATTCTTAATGTTACACCGGATTCATTTTCAGAT\n",
      "\n",
      "Seq1 GGCGGAAAATATGACAGCTTGGACAAGGCGCTGCTGCACGCGAAAGAGATGATCGATGAT\n",
      "       *           ***** *  *  *  *  *  *  *  *     *     *      \n",
      "Seq2 GGAGGAAAATATGATTCACTTGATAAAGCTCTTCTTCATGCTAAAGAAATGATTGATGAT\n",
      "\n",
      "Seq1 GGTGCCCATATCATTGATATTGGAGGGGAATCGACAAGGCCTGGCGCTGAGTGCGTATCT\n",
      "       *  *     *              *     *   * *  *  *     *  *  *  *\n",
      "Seq2 GGAGCTCATATTATTGATATTGGAGGAGAATCAACACGTCCGGGAGCTGAATGTGTTTCA\n",
      "\n",
      "Seq1 GAGGATGAGGAGATGTCCAGAGTCATTCCGGTGATTGAGCGGATTACGAAAGAGCTTGGT\n",
      "       *     *  *     ** *  *        *     *  *     *     *     *\n",
      "Seq2 GAAGATGAAGAAATGTCACGTGTTATTCCGGTTATTGAACGTATTACAAAAGAACTTGGA\n",
      "\n",
      "Seq1 GTTCCTATTTCTGTAGACACGTACAAGGCTTCTGTCGCAGATGAAGCAGTGAAAGCCGGT\n",
      "          *     *  *  *  *  *  *     *  *  *        *  *     *  *\n",
      "Seq2 GTTCCGATTTCAGTTGATACATATAAAGCTTCAGTTGCTGATGAAGCTGTTAAAGCTGGA\n",
      "\n",
      "Seq1 GCATCCATTATCAATGATATTTGGGGAGCCAAACATGATCCGAAGATGGCTTCCGTTGCA\n",
      "       *  *     *                 *              *        *     *\n",
      "Seq2 GCTTCAATTATTAATGATATTTGGGGAGCTAAACATGATCCGAAAATGGCTTCAGTTGCT\n",
      "\n",
      "Seq1 GCTGAACATAATGTTCCAATTGTACTCATGCATAACCGCCCTGAAAGAAACTACAATGAC\n",
      "                      *     *  *        *  *  *   * *  *  *     *\n",
      "Seq2 GCTGAACATAATGTTCCGATTGTTCTTATGCATAATCGTCCGGAACGTAATTATAATGAT\n",
      "\n",
      "Seq1 TTATTGCCGGATATGCTGTCGGACTTAATGGAGAGTGTAAAAATTGCTGTTGAGGCCGGA\n",
      "     * ** *           *  *  ** *     ****  *              *  *   \n",
      "Seq2 CTTCTTCCGGATATGCTTTCAGATCTTATGGAATCAGTTAAAATTGCTGTTGAAGCTGGA\n",
      "\n",
      "Seq1 GTAGACGAGAAGAACATTATTCTTGATCCTGGTATCGGTTTCGCGAAAACCTATCACGAT\n",
      "       *  *  *  *  *              *  *  *  *  *  *     *     *   \n",
      "Seq2 GTTGATGAAAAAAATATTATTCTTGATCCGGGAATTGGATTTGCTAAAACATATCATGAT\n",
      "\n",
      "Seq1 AACTTGGCAGTGATGAACAAACTAGAGATTTTCAGCGGATTGGGATATCCGGTTCTTCTG\n",
      "       ** *  *  *     *     *  *     ****   * *                 *\n",
      "Seq2 AATCTTGCTGTTATGAATAAACTTGAAATTTTTTCAGGACTTGGATATCCGGTTCTTCTT\n",
      "\n",
      "Seq1 GCAACCTCCCGAAAAAGATTCATCGGACGTGTTCTGGATCTTCCGCCTGAGGAGCGGGCT\n",
      "       *  *  *  *   * *  *  *           *           *  *  *  *   \n",
      "Seq2 GCTACATCACGTAAACGTTTTATTGGACGTGTTCTTGATCTTCCGCCGGAAGAACGTGCT\n",
      "\n",
      "Seq1 GAGGGCACAGGCGCGACTGTGTGTCTCGGCATTCAAAAAGGCTGTGACATTGTCAGGGTC\n",
      "       *  *     *  *  *  *     *  *           *     *     ** *  *\n",
      "Seq2 GAAGGAACAGGAGCTACAGTTTGTCTTGGAATTCAAAAAGGATGTGATATTGTTCGTGTT\n",
      "\n",
      "Seq1 CATGATGTAAAGCAAATTGCCAGAATGGCGAAAATGATGGACGCGATGCTGAATAAGGGA\n",
      "             *  *        ** *     *           *  *     *     *   \n",
      "Seq2 CATGATGTTAAACAAATTGCTCGTATGGCTAAAATGATGGATGCTATGCTTAATAAAGGA\n",
      "\n",
      "Seq1 GGGGTGCACCATGGATAA\n",
      "       *  *  *         \n",
      "Seq2 GGAGTTCATCATGGATAA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mle_dna = get_most_common_codons(test_protein, cu_table)\n",
    "align(test_dna, mle_dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "together-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN fraction bps matching: 0.769\n",
      "MLE fraction bps matching: 0.780\n"
     ]
    }
   ],
   "source": [
    "def fraction_matches(seq1, seq2):\n",
    "    '''Compare sequences in terms of matching positions.'''\n",
    "    matches = 0\n",
    "    for i, c1 in enumerate(seq1):\n",
    "        if c1 == seq2[i]:\n",
    "            matches += 1\n",
    "    return {'length':len(seq1), \n",
    "            'n_matches':matches, \n",
    "            'n_mismatches':len(seq1) - matches, \n",
    "            'fraction_matches':matches/len(seq1)}\n",
    "\n",
    "fm_rnn = fraction_matches(test_dna, pred_dna)\n",
    "fm_mle = fraction_matches(test_dna, mle_dna)\n",
    "\n",
    "print(f'RNN fraction bps matching: {fm_rnn[\"fraction_matches\"]:.3f}')\n",
    "print(f'MLE fraction bps matching: {fm_mle[\"fraction_matches\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-force",
   "metadata": {},
   "source": [
    "305 out of 500 positions match (61%). But 195 do not match. Is that good? Did the model learn anything? Is it approximating our HMM better than a random H and L generator would?\n",
    "\n",
    "Keep in mind that the HMM training examples were draw probabalistically, so there is noise in the training data to begin with. That is, we don't expect perfect alignment. However, it is reasonable to simulate how often random draws of H and L sequences of length 500 would be expected to match at 61% or better. It turns out that even simulating 1000 random H and L sequence pairs, the best matches never exceed ~58%. \n",
    "\n",
    "So, 61% match is exceedingly rare by random chance. We can safely conclude that our deep network learned to approximate our HMM.\n",
    "\n",
    "It's also interesting to see whether the probabilities are more or less \"confident\" at matches vs mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probs(true_seq, pred_seq, prob, symbols, n=15):\n",
    "    print('Pos True Pred Prob:(' + ' '.join(symbols) + ')')\n",
    "    for i in range(n):\n",
    "        ind = ' '\n",
    "        if true_seq[i] != pred_seq[i]:\n",
    "            ind = '*'\n",
    "        prob_str = ''\n",
    "        for j in range(len(symbols)):\n",
    "            prob_str += f'{pred_prob_hl[i, j]:.2f} '\n",
    "        print(f'{i}   {true_seq[i]}  {ind} {pred_seq[i]}    ' + prob_str)\n",
    "\n",
    "print_probs(test_hlseq, pred_hl, pred_prob_hl, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-azerbaijan",
   "metadata": {},
   "source": [
    "Based on the first 15 positions, the mismatches don't seem any less \"confident\" than the matches. For example, at position 0 the model correctly predicted \"H\" with 57% confidence. At position 2, the model incorrectly predicted \"H\", but still had 58% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-giant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
