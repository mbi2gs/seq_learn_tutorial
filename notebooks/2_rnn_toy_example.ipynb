{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flush-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-invalid",
   "metadata": {},
   "source": [
    "In this notebook we go through the process of training a recurrent neural network (RNN) to learn the same distribution that our toy HMM follows.\n",
    "\n",
    "First, some helper functions to encode and decode sequences. Remember, `assert` statements are little tests to make sure that things are working the way we expect. They are *very* helpful for catching silly bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "experienced-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['H', 'L']\n",
    "nucleotides = ['A', 'C', 'G', 'T']\n",
    "\n",
    "def encode_seq(symbols, seqtype='dna'):\n",
    "    encdr = nucleotides\n",
    "    if seqtype != 'dna':\n",
    "        encdr = states\n",
    "    outseq = np.array([encdr.index(s) for s in symbols])\n",
    "    return outseq\n",
    "\n",
    "test_hl = 'HHHLLL'\n",
    "test_nuc = 'GGGAAA'\n",
    "assert encode_seq(test_hl, seqtype='states')[0] == states.index(test_hl[0]) and \\\n",
    "       encode_seq(test_hl, seqtype='states')[-1] == states.index(test_hl[-1])\n",
    "assert encode_seq(test_nuc, seqtype='dna')[0] == nucleotides.index(test_nuc[0]) and \\\n",
    "       encode_seq(test_nuc, seqtype='dna')[-1] == nucleotides.index(test_nuc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neutral-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(num_array, seqtype='dna'):\n",
    "    encdr = nucleotides\n",
    "    if seqtype != 'dna':\n",
    "        encdr = states\n",
    "    outseq = [encdr[s] for s in num_array]\n",
    "    return ''.join(outseq)\n",
    "\n",
    "assert decode_seq(encode_seq(test_nuc)) == test_nuc\n",
    "assert decode_seq(encode_seq(test_hl, seqtype='hid'), seqtype='hid') == test_hl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-clinton",
   "metadata": {},
   "source": [
    "Now we read in the training data emitted by the HMM in notebook 0. Let's split it in half, so that we have some sequences for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coastal-philippines",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dna</th>\n",
       "      <th>hidden_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TGGTCGTATTTTGTCGGGGGCAGACCAAAAAACAACGAAACGAATG...</td>\n",
       "      <td>LLLLLLLHHLLLHHLHHHLLHLLHHLLHLLHLHLLLLLHHHHLLLH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCACGGTGGATGTATCGCTGTGCAAGCAAGCCGGGATACTGCTTGT...</td>\n",
       "      <td>HHHHHHHHHHLHLHHHLLLHLHHLHHHHLLHLHLLLHHLLLLHHLH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 dna  \\\n",
       "0  TGGTCGTATTTTGTCGGGGGCAGACCAAAAAACAACGAAACGAATG...   \n",
       "1  GCACGGTGGATGTATCGCTGTGCAAGCAAGCCGGGATACTGCTTGT...   \n",
       "\n",
       "                                        hidden_state  \n",
       "0  LLLLLLLHHLLLHHLHHHLLHLLHHLLHLLHLHLLLLLHHHHLLLH...  \n",
       "1  HHHHHHHHHHLHLHHHLLLHLHHLHHHHLLHLHLLLHHLLLLHHLH...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_file = 'rnn_toy_training.tsv'\n",
    "training_df = pd.read_csv(training_data_file, sep='\\t')\n",
    "training_df = training_df.head(50)\n",
    "testing_df = training_df.tail(50)\n",
    "training_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-creek",
   "metadata": {},
   "source": [
    "Our data set will consist of PyTorch tensors. These are simply numerical matrices (like you would find in Numpy or MATLAB or R), but they include the ability to track gradients.\n",
    "\n",
    "We have 50 training example sequences, each of length 500. We first encode the strings into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "standing-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = training_df.dna.str.len().max()\n",
    "NUM_SEQS = training_df.shape[0]\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "X_train = torch.zeros(NUM_SEQS, SEQ_LEN, dtype=torch.long)\n",
    "Y_train = torch.zeros(NUM_SEQS, SEQ_LEN, dtype=torch.long)\n",
    "for i, row in training_df.iterrows():\n",
    "    dna = row['dna']\n",
    "    hid = row['hidden_state']\n",
    "    dna_encode = torch.LongTensor(encode_seq(dna, seqtype='dna'))\n",
    "    hid_encode = torch.LongTensor(encode_seq(hid, seqtype='hid'))\n",
    "\n",
    "    X_train[i, :] = dna_encode\n",
    "    Y_train[i, :] = hid_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advanced-recipient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 500]), torch.Size([50, 500]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-robertson",
   "metadata": {},
   "source": [
    "Next, we load the data into a `Dataset` and `Dataloader` module. This is not strictly necessary for training, but it makes it easier to shuffle, sample from and batch the data. When projects get more complicated, these modules are very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "patient-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-criminal",
   "metadata": {},
   "source": [
    "Here we make sure the data comes back out of the dataloader in the way we expect. The `batch_size` refers to the number of examples sampled simultaneously. In this case, we only retrieve one example sequence at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "valid-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 500]), torch.Size([1, 500]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the data comes back out in the way we expect\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-beaver",
   "metadata": {},
   "source": [
    "Here we set up a [python class](https://www.geeksforgeeks.org/python-classes-and-objects/) to manage the various pieces of our model. Again, this is not strictly necessary, but it makes life simpler. For example, the class will track useful information internally (e.g. `input_size`, `n_layers`). We can also create convenience functions such as `init_hidden()` to create a fresh hidden layer without having to remember what the precise dimensions ought to be. \n",
    "\n",
    "There are two pieces to our class. The `__init__()` function that is run when we first instantiate an instance of `MyGruClass`. This is where we initialize variables with the correct values, and instantiate the machine learning layers. A Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN). Notice the Tensorflow GRU module is called here (which is the heart of our model). We also include an [\"embedding\" layer](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526), which learns to represent our integer inputs (remember we converted our sequence of characters into a sequence of integers using the `encode_seq()` function) as a vector of foating point numbers. That vector of floating point numbers serves as the input to our GRU. Finally, we instantiate a `Linear` and `LogSoftMax` layer, both of which convert the GRU output into a sequence of log probabilities. \n",
    "\n",
    "The `forward()` function is where we actually *use* the layers we created with `__init()`. In `forward()` we take an input and cascade it through the layers to produce an output. The key to any kind of deep learning project is to carefully track the input and output dimensions of your layers. Notice my comments to help myself mentally track what each layer is spitting out, and what the next layer expects. It helps to be aware of the [`transpose()` function](https://pytorch.org/docs/stable/generated/torch.transpose.html), which allows you to rotate two dimensions. This helps to match a tensor with the input expections of a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acceptable-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGruClass(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, predict_size, n_layers=1, bdir=False):\n",
    "        super(MyGruClass, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.predict_size = predict_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bdir else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, self.embed_size)\n",
    "        self.gru = nn.GRU(self.embed_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=n_layers, \n",
    "                          bidirectional=bdir)\n",
    "\n",
    "        self.lin_out = nn.Linear(hidden_size*self.n_directions, predict_size)\n",
    "        self.sigmoid = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        # embedding shape: (batch_size, seq_len, hidden_size)\n",
    "        # transpose so that batch dim is in the 2nd index position\n",
    "        output = torch.transpose(embedded, 0, 1)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output shape: (seq_len, batch_size, n_directions*hidden_size)\n",
    "        # hidden shape: (n_directions*n_layers, batch_size, hidden_size)\n",
    "        \n",
    "        output = self.sigmoid(self.lin_out(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(self.n_layers*self.n_directions, \n",
    "                           batch_size, \n",
    "                           self.hidden_size)\n",
    "\n",
    "    def input_dims(self):\n",
    "        print(f'Input dimensions are: (batch_size, seq_len, {self.input_size})')\n",
    "    \n",
    "    def output_dims(self):\n",
    "        print(f'Output dimensions are: (seq_len, batch_size, {self.predict_size})')\n",
    "    \n",
    "    def hidden_dims(self):\n",
    "        dnl = self.n_layers*self.n_directions\n",
    "        print(f'Hidden dimensions are: ({dnl}, batch_size, {self.hidden_size})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amino-publicity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions are: (batch_size, seq_len, 4)\n",
      "Hidden dimensions are: (1, batch_size, 10)\n",
      "Output dimensions are: (seq_len, batch_size, 2)\n"
     ]
    }
   ],
   "source": [
    "# Using convenience functions in MyGruClass to print expected dimensions\n",
    "test_model = MyGruClass(len(nucleotides), 10, len(states))\n",
    "test_model.input_dims(), test_model.hidden_dims(), test_model.output_dims();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-problem",
   "metadata": {},
   "source": [
    "The `train()` function is where the action happens. Here we instantiate our model, and start feeding data to it. We select the Negative Log-Likelihood Loss `NLLLoss()` [function](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) as our optimization function because it determines whether a log probality (the output of our GRU) correctly classifies the input. In other words, the more probability the model assigns to the correct category, the more the the model is \"rewarded\". \n",
    "\n",
    "In our case, the model will output a vector of two probabilities (log transformed), one for `H` and one for `L`. For every input nucleotide, the model will assign the probability that it corresponds to a \"high GC\" region or a \"low GC\" region. The training data also has the answer generated by the HMM to which the model can compare its prediction. The more probability assigned to the correct category, the more the current model weights are reinforced.\n",
    "\n",
    "There are many loss functions, all intended for different scenarios or that have different emphases. When starting a new project, it's worth reviewing the [available loss functions](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7) to pick the one or two that seem most appropriate.\n",
    "\n",
    "PyTorch also offers [multiple optimization algorithms](https://pytorch.org/docs/stable/optim.html). For this project we went with a common default: Adam. \n",
    "\n",
    "Having initialized training data, a model, a loss function, and an optimizer, we are ready to learn. The training process proceeds to loop through the data set in a random order (because we set the `shuffle` parameter on our Dataloader to `True`). A full loop through the data is called an \"epoch\". Within each epoch, we iterate through every training batch (in this case, batches are just one sequence long). Before feeding a sequence to our RNN, we reset the hidden state and the gradient. We then feed the training sequence to our model, and collect the prediction output. The output is then reorganized (using `transpose()`) to match the input expections of our NLLLoss function. Once we get a loss value, we call `backward()` to calculate derivatives, and which are fed to the optimization function, which updates the model weights. It's remarkable how much PyTorch keeps track of for us.\n",
    "\n",
    "Finally, there are some print statements to keep track of where we are in the loop, and whether the model is continuing to improve or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "purple-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, \n",
    "          learn_rate=0.02, \n",
    "          input_dim=len(nucleotides), \n",
    "          hidden_dim=10,\n",
    "          output_dim=len(states),\n",
    "          batch_size=1,\n",
    "          EPOCHS=5):\n",
    "    \n",
    "    # Instantiating the model\n",
    "    model = MyGruClass(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting training\")\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        start_time = time.time()\n",
    "        avg_loss = 0\n",
    "        for sample_x, sample_y in train_loader:\n",
    "            h = model.init_hidden(batch_size)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # the heart of the training!\n",
    "            out, h = model(sample_x, h)\n",
    "            \n",
    "            # NLLLoss expects batch first, then class probabilities, then seq_len\n",
    "            out_T = torch.transpose(out, 0, 1)\n",
    "            out_T = out_T.transpose(1, 2)\n",
    "\n",
    "            loss = criterion(out_T, sample_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "        current_time = time.time()\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} Done, Total Loss: {avg_loss/len(train_loader):.3f}\")\n",
    "        print(f\"Total Time Elapsed: {current_time-start_time:.1f} seconds\")\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(f\"Total Training Time: {sum(epoch_times):.1f} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-torture",
   "metadata": {},
   "source": [
    "And now, for the big moment. We train the model! \n",
    "\n",
    "This will take a about 30 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "particular-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/5 Done, Total Loss: 0.671\n",
      "Total Time Elapsed: 5.1 seconds\n",
      "Epoch 2/5 Done, Total Loss: 0.667\n",
      "Total Time Elapsed: 5.1 seconds\n",
      "Epoch 3/5 Done, Total Loss: 0.668\n",
      "Total Time Elapsed: 5.1 seconds\n",
      "Epoch 4/5 Done, Total Loss: 0.667\n",
      "Total Time Elapsed: 5.1 seconds\n",
      "Epoch 5/5 Done, Total Loss: 0.667\n",
      "Total Time Elapsed: 5.1 seconds\n",
      "Total Training Time: 25.6 seconds\n"
     ]
    }
   ],
   "source": [
    "gru_model = train(train_loader, learn_rate = 0.02, EPOCHS=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-painting",
   "metadata": {},
   "source": [
    "The model does improve the loss function in the first three epochs, then plateaus.\n",
    "\n",
    "Let's see how the GRU's predictions compare to a real example!\n",
    "\n",
    "First, we need a function to handle all the steps of encoding and decoding the output and resetting the model. This function outputs both the predicted state (a string of H's and L's), and the associated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "textile-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dna):\n",
    "    assert all([x in nucleotides for x in dna])\n",
    "    assert isinstance(model, MyGruClass)\n",
    "    dna_encode = torch.LongTensor(encode_seq(dna, seqtype='dna'))\n",
    "    dna_encode = dna_encode[None, :]\n",
    "    h = model.init_hidden(1)\n",
    "    model.zero_grad()\n",
    "    out, _ = model(dna_encode, h)\n",
    "    out_state_indices = [int(torch.argmax(x)) for x in out[:,0]]\n",
    "    out_probs = np.array([torch.exp(x).detach().numpy() for x in out[:,0]])\n",
    "    state = decode_seq(out_state_indices, 'hid')\n",
    "    return state, out_probs\n",
    "\n",
    "test_seq = 'GGGTTT'\n",
    "test_state, test_probs = predict(gru_model, test_seq)\n",
    "assert len(test_state) == len(test_seq)\n",
    "assert all([x in states for x in test_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-heater",
   "metadata": {},
   "source": [
    "Here we pick a sequence from our testing data that the model has never seen. We can align the prediction and the HMM generated sequence to see how closely they agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "civil-still",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq1 HLLLLLLLLHLLHLHHLLLLLLHLLLHHLHHLHLLLLHLLLLHHHLLLLHLHLLLLLLLH\n",
      "       *  *     *  ***  ****   *   *   ** *  *      *     **** * \n",
      "Seq2 HLHLLHLLLHLHHLLLHLLHHHLLLLLHLHLLHLHHLLLLHLHHHLLHLHLHLHHHHLHH\n",
      "\n",
      "Seq1 HHLLLHHLHHLHLHLLHHHLHHHLHHHHHLLLLLHLLHHHLLHLLHHLLLHHLHHHLLLH\n",
      "     ***   ** ** *  ** * * *    *   *   *   *  **     ** *       \n",
      "Seq2 LLHLLHLHHLHHHHLHLHLLLHLLHHHLHLLHLLHHLHHLLLLHLHHLLHLHHHHHLLLH\n",
      "\n",
      "Seq1 LLLLLLHHHHLLLHLLLHHHHLLHLLLLLLLHHHLLLLLLHHLLHHLLLLLHHHHLHHLH\n",
      "      ** ***** * *    **  **   *  *    ***** * **  * *  ** * *   \n",
      "Seq2 LHHLHHLLLHHLHHLLLLLHHHHHLLHLLHLHHHHHHHHLLHHHHHHLHLLLLHLLLHLH\n",
      "\n",
      "Seq1 LHHHLHHHHHLLLLLLHHHHLHHLHLLLHLLLLLLLHHHLHHHLLHLLHLHHHLLLHLLL\n",
      "       ** ** *    *      ** ** *   * *   * *  *      **  * * **  \n",
      "Seq2 LHLLLLLHLHLLLHLLHHHHHLHHLLHLHLHLHLLLLHLLHLHLLHLLLHHHLLHLLHLL\n",
      "\n",
      "Seq1 HHHLHLHLLLLHHHHHHHHHHHHLHHLLLLLLHLLHHHLHLHHHLLHLHHHHHLLHHLHH\n",
      "      ** *       *   * *   *    *    *  *   ****  **   **  ** *  \n",
      "Seq2 HLLLLLHLLLLHLHHHLHLHHHLLHHLHLLLLLLLLHHLLHLLHLHLLHHLLHLHLHHHH\n",
      "\n",
      "Seq1 LHHLHLHHLLLLLHLHHHLLLLHLLLHHLLLLLHLHHLLHHHLHLLLHHLLLHLLLHLHH\n",
      "         **  *       *   ****   *******  **   ** *      * **   **\n",
      "Seq2 LHHLLHHHHLLLLHLHLHLLHHLHLLHLHHHHHLLHLHLHHLHHHLLHHLLHHHHLHLLL\n",
      "\n",
      "Seq1 HLLHLHLHHHLHHLLLHHHLLHHHHLLLLLHLLLLLLLLHLLLLHHHLHLLHLLLLLHHL\n",
      "     **   ***    **   ** * ** *     **   *  *** * *      *** * * \n",
      "Seq2 LHLHLLHLHHLHLHLLHLLLHHLLHHLLLLHHHLLLHLLLHHLHHLHLHLLHHHHLHHLL\n",
      "\n",
      "Seq1 HLHLLLLHHLLLLLLLLLLLLHHHHHHLHHLLLLHLLLHHLLHLLHLLHHLLHHLHLHLL\n",
      "         * * **    *  **              * *     *** *   * *** *  * \n",
      "Seq2 HLHLHLHHLHLLLLHLLHHLLHHHHHHLHHLLLHHHLLHHLHLHLLLLHLLHLLLLLHHL\n",
      "\n",
      "Seq1 LHLHLLHLLLHHHHHLLLLH\n",
      "      *** *   **** *     \n",
      "Seq2 LLHLLHHLLHLLLHLLLLLH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_nucleotides = testing_df.iloc[-1, 0]\n",
    "test_hlseq = testing_df.iloc[-1, 1]\n",
    "pred_hl, pred_prob_hl = predict(gru_model, test_nucleotides)\n",
    "\n",
    "def align(seq1, seq2, WIDTH=60):\n",
    "    '''Align two input sequences of equal length,\n",
    "    with *  between indicating mismatches.'''\n",
    "    lines = int(np.ceil(len(seq1) / WIDTH))\n",
    "    match = ''\n",
    "    for i, c1 in enumerate(seq1):\n",
    "        indicator = ' '\n",
    "        if c1 != seq2[i]:\n",
    "            indicator = '*'\n",
    "        match += indicator\n",
    "    \n",
    "    for i in range(lines):\n",
    "        print('Seq1', seq1[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('    ', match[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print('Seq2', seq2[i*WIDTH:i*WIDTH+WIDTH])\n",
    "        print()\n",
    "\n",
    "align(test_hlseq, pred_hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "informational-hurricane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length': 500,\n",
       " 'n_matches': 305,\n",
       " 'n_mismatches': 195,\n",
       " 'fraction_matches': 0.61,\n",
       " 'prob_of_rand_match_fraction': '<0.001'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fraction_matches(seq1, seq2):\n",
    "    '''Compare sequences in terms of matching positions.'''\n",
    "    matches = 0\n",
    "    for i, c1 in enumerate(seq1):\n",
    "        if c1 == seq2[i]:\n",
    "            matches += 1\n",
    "    return {'length':len(seq1), \n",
    "            'n_matches':matches, \n",
    "            'n_mismatches':len(seq1) - matches, \n",
    "            'fraction_matches':matches/len(seq1)}\n",
    "\n",
    "def sim_prob_match(fraction, symbols, length, n_draws=1000):\n",
    "    '''Simulate n_draws pairs of random sequences composed of symbols.\n",
    "    Estimate likelihood of matching at or above the input fraction \n",
    "    of positions.'''\n",
    "    fraction_distribution = []\n",
    "    for i in range(n_draws):\n",
    "        seqs = np.random.choice(symbols, size=[2, length], replace=True)\n",
    "        match = fraction_matches(''.join(seqs[0,:]), ''.join(seqs[1,:]))\n",
    "        fraction_distribution.append(match['fraction_matches'])\n",
    "    tail = np.sum(np.array(fraction_distribution) > fraction)\n",
    "    max_fraction = np.max(np.array(fraction_distribution))\n",
    "    prob_of_random_match_fraction = tail / n_draws\n",
    "    display_p = f'{prob_of_random_match_fraction:.2f}'\n",
    "    if prob_of_random_match_fraction == 0:\n",
    "        display_p = f'<{1.0/n_draws}'\n",
    "    return {'pval':prob_of_random_match_fraction,\n",
    "            'max_fraction':max_fraction,\n",
    "            'display_p':display_p}\n",
    "\n",
    "fm = fraction_matches(test_hlseq, pred_hl)\n",
    "spm = sim_prob_match(0.61, states, length=500, n_draws=1000)\n",
    "\n",
    "fm['prob_of_rand_match_fraction'] = spm['display_p']\n",
    "fm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-queens",
   "metadata": {},
   "source": [
    "305 out of 500 positions match (61%). But 195 do not match. Is that good? Did the model learn anything? Is it approximating our HMM better than a random H and L generator would?\n",
    "\n",
    "Keep in mind that the HMM training examples were draw probabalistically, so there is noise in the training data to begin with. That is, we don't expect perfect alignment. However, it is reasonable to simulate how often random draws of H and L sequences of length 500 would be expected to match at 61% or better. It turns out that even simulating 1000 random H and L sequence pairs, the best matches never exceed ~58%. \n",
    "\n",
    "So, 61% match is exceedingly rare by random chance. We can safely conclude that our deep network learned to approximate our HMM.\n",
    "\n",
    "It's also interesting to see whether the probabilities are more or less \"confident\" at matches vs mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "consolidated-difficulty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos True Pred Prob:(H L)\n",
      "0   H    H    0.57 0.43 \n",
      "1   L    L    0.34 0.66 \n",
      "2   L  * H    0.56 0.44 \n",
      "3   L    L    0.36 0.64 \n",
      "4   L    L    0.33 0.67 \n",
      "5   L  * H    0.53 0.47 \n",
      "6   L    L    0.33 0.67 \n",
      "7   L    L    0.31 0.69 \n",
      "8   L    L    0.31 0.69 \n",
      "9   H    H    0.51 0.49 \n",
      "10   L    L    0.32 0.68 \n",
      "11   L  * H    0.55 0.45 \n",
      "12   H    H    0.59 0.41 \n",
      "13   L    L    0.36 0.64 \n",
      "14   H  * L    0.33 0.67 \n"
     ]
    }
   ],
   "source": [
    "def print_probs(true_seq, pred_seq, prob, symbols, n=15):\n",
    "    print('Pos True Pred Prob:(' + ' '.join(symbols) + ')')\n",
    "    for i in range(n):\n",
    "        ind = ' '\n",
    "        if true_seq[i] != pred_seq[i]:\n",
    "            ind = '*'\n",
    "        prob_str = ''\n",
    "        for j in range(len(symbols)):\n",
    "            prob_str += f'{pred_prob_hl[i, j]:.2f} '\n",
    "        print(f'{i}   {true_seq[i]}  {ind} {pred_seq[i]}    ' + prob_str)\n",
    "\n",
    "print_probs(test_hlseq, pred_hl, pred_prob_hl, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-tribe",
   "metadata": {},
   "source": [
    "Based on the first 15 positions, the mismatches don't seem any less \"confident\" than the matches. For example, at position 0 the model correctly predicted \"H\" with 57% confidence. At position 2, the model incorrectly predicted \"H\", but still had 58% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-rings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
